{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Commands/Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:26.186792Z",
     "start_time": "2020-07-08T19:57:26.182804Z"
    }
   },
   "outputs": [],
   "source": [
    "### FOR NON-ANACONDA PYSPARK\n",
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:26.434131Z",
     "start_time": "2020-07-08T19:57:26.189785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:38.998529Z",
     "start_time": "2020-07-08T19:57:26.437123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create rudimentary dataframe\n",
    "df = spark.sql(\"\"\"select 'spark' as hello \"\"\")\n",
    "\n",
    "# Display dataframe\n",
    "df.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:46.718278Z",
     "start_time": "2020-07-08T19:57:39.001521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1412\n"
     ]
    }
   ],
   "source": [
    "# Function to compute pi\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "num_samples = 10000\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Basics\n",
    "\n",
    "Material from [TowardsDataScience](https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:47.094153Z",
     "start_time": "2020-07-08T19:57:46.720153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date, timedelta, datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:47.329524Z",
     "start_time": "2020-07-08T19:57:47.096147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "sc = SparkSession.builder.appName(\"PySparkExample\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"5g\")\\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:50.450214Z",
     "start_time": "2020-07-08T19:57:47.332518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "|                 _id|  amazon_product_url|              author| bestsellers_date|         description|   price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]|[[1212883200000]]|       Bantam| [1]|           [0]|           ODD HOURS|          [1]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|Little, Brown| [2]|           [1]|            THE HOST|          [3]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]| St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]|[[1212883200000]]|       Putnam| [4]|           [0]|           THE FRONT|          [1]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]|[[1212883200000]]|    Doubleday| [5]|           [0]|               SNUFF|          [1]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|[[1211587200000]]|A woman finds an ...|[24.99,]|[[1212883200000]]|Little, Brown| [6]|           [3]|SUNDAYS AT TIFFANY’S|          [4]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       John Sandford|[[1211587200000]]|The Minneapolis d...|[26.95,]|[[1212883200000]]|       Putnam| [7]|           [4]|        PHANTOM PREY|          [3]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|[[1211587200000]]|A Southern family...|[21.99,]|[[1212883200000]]|Little, Brown| [8]|           [6]|          SWINE NOT?|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|[[1211587200000]]|In Cornwall, tryi...|[27.95,]|[[1212883200000]]|       Harper| [9]|           [8]|     CARELESS IN RED|          [3]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|[[1211587200000]]|An intelligence a...|[26.99,]|[[1212883200000]]|Grand Central|[10]|           [7]|     THE WHOLE TRUTH|          [5]|\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read JSON file to dataframe\n",
    "dataframe = sc.read.json(os.path.join(\"02-raw-data\", \"nyt2.json\"))\n",
    "\n",
    "# Show first 10 rows\n",
    "dataframe.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:50.458194Z",
     "start_time": "2020-07-08T19:57:50.453206Z"
    }
   },
   "outputs": [],
   "source": [
    "# JSON\n",
    "# dataframe = sc.read.json('dataset/nyt2.json')\n",
    "\n",
    "# TXT FILES\n",
    "# dataframe_txt = sc.read.text('text_data.txt')\n",
    "\n",
    "# CSV FILES\n",
    "# dataframe_csv = sc.read.csv('csv_data.csv')\n",
    "\n",
    "# PARQUET FILES\n",
    "# dataframe_parquet = sc.read.load('parquet_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:51.580209Z",
     "start_time": "2020-07-08T19:57:50.460187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+----------------+----+--------------+--------------------+-------------+\n",
      "|                 _id|  amazon_product_url|              author| bestsellers_date|         description|   price|   published_date|       publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+----------------+----+--------------+--------------------+-------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       John Sandford|[[1212192000000]]|The Minneapolis d...|[26.95,]|[[1213488000000]]|          Putnam| [9]|           [7]|        PHANTOM PREY|          [4]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Sebastian Faulks|[[1212796800000]]|James Bond tracks...|[24.95,]|[[1214092800000]]|       Doubleday|[11]|           [8]|      DEVIL MAY CARE|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|[[1213401600000]]|A woman’s happy m...|[24.95,]|[[1214697600000]]|    St. Martin’s| [5]|           [4]|LOVE THE ONE YOU'...|          [5]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|   Lauren Weisberger|[[1214006400000]]|Three glamorous f...|[25.95,]|[[1215302400000]]|Simon & Schuster| [4]|           [6]|CHASING HARRY WIN...|          [4]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|[[1216425600000]]|A sailing vacatio...|[27.99,]|[[1217721600000]]|   Little, Brown| [6]|           [6]|                SAIL|          [6]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       E Lynn Harris|[[1217030400000]]|Secrets threaten ...|[24.95,]|[[1218326400000]]|      Double­day|[13]|           [8]|JUST TOO GOOD TO ...|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|          Jane Green|[[1217030400000]]|A woman rents out...|[24.95,]|[[1218326400000]]|          Viking|[16]|          [14]|     THE BEACH HOUSE|          [6]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|   Christopher Reich|[[1218240000000]]|The death of a su...|   [, 0]|[[1219536000000]]|       Doubleday|[19]|           [0]|  RULES OF DECEPTION|          [0]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    David Wroblewski|[[1223683200000]]|A mute takes refu...|[25.95,]|[[1224979200000]]|            Ecco| [2]|           [2]|THE STORY OF EDGA...|         [18]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Mary Ann Shaffer ...|[[1223683200000]]|A journalist meet...|  [, 22]|[[1224979200000]]|            Dial|[15]|          [12]|THE GUERNSEY LITE...|         [11]|\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+----------------+----+--------------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "dataframe_dropdup = dataframe.dropDuplicates()\n",
    "dataframe_dropdup.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:51.735759Z",
     "start_time": "2020-07-08T19:57:51.583201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              author|\n",
      "+--------------------+\n",
      "|       Dean R Koontz|\n",
      "|     Stephenie Meyer|\n",
      "|        Emily Giffin|\n",
      "|   Patricia Cornwell|\n",
      "|     Chuck Palahniuk|\n",
      "|James Patterson a...|\n",
      "|       John Sandford|\n",
      "|       Jimmy Buffett|\n",
      "|    Elizabeth George|\n",
      "|      David Baldacci|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select author data\n",
    "dataframe.select(\"author\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:51.904736Z",
     "start_time": "2020-07-08T19:57:51.738752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+--------+\n",
      "|              author|               title|rank|   price|\n",
      "+--------------------+--------------------+----+--------+\n",
      "|       Dean R Koontz|           ODD HOURS| [1]|  [, 27]|\n",
      "|     Stephenie Meyer|            THE HOST| [2]|[25.99,]|\n",
      "|        Emily Giffin|LOVE THE ONE YOU'...| [3]|[24.95,]|\n",
      "|   Patricia Cornwell|           THE FRONT| [4]|[22.95,]|\n",
      "|     Chuck Palahniuk|               SNUFF| [5]|[24.95,]|\n",
      "|James Patterson a...|SUNDAYS AT TIFFANY’S| [6]|[24.99,]|\n",
      "|       John Sandford|        PHANTOM PREY| [7]|[26.95,]|\n",
      "|       Jimmy Buffett|          SWINE NOT?| [8]|[21.99,]|\n",
      "|    Elizabeth George|     CARELESS IN RED| [9]|[27.95,]|\n",
      "|      David Baldacci|     THE WHOLE TRUTH|[10]|[26.99,]|\n",
      "+--------------------+--------------------+----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select title, author, rank, and price data\n",
    "dataframe.select(\"author\", \"title\", \"rank\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:52.140758Z",
     "start_time": "2020-07-08T19:57:51.907728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------------------------------------+\n",
      "|               title|CASE WHEN (NOT (title = ODD HOURS)) THEN 1 ELSE 0 END|\n",
      "+--------------------+-----------------------------------------------------+\n",
      "|           ODD HOURS|                                                    0|\n",
      "|            THE HOST|                                                    1|\n",
      "|LOVE THE ONE YOU'...|                                                    1|\n",
      "|           THE FRONT|                                                    1|\n",
      "|               SNUFF|                                                    1|\n",
      "|SUNDAYS AT TIFFANY’S|                                                    1|\n",
      "|        PHANTOM PREY|                                                    1|\n",
      "|          SWINE NOT?|                                                    1|\n",
      "|     CARELESS IN RED|                                                    1|\n",
      "|     THE WHOLE TRUTH|                                                    1|\n",
      "+--------------------+-----------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show title and when associated with title\n",
    "dataframe.select(\"title\", when(dataframe.title !=\n",
    "                               'ODD HOURS', 1).otherwise(0)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:52.379633Z",
     "start_time": "2020-07-08T19:57:52.143751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+-----------------+--------------------+--------+-----------------+------------+----+--------------+--------------------+-------------+\n",
      "|                 _id|  amazon_product_url|      author| bestsellers_date|         description|   price|   published_date|   publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+--------------------+--------------------+------------+-----------------+--------------------+--------+-----------------+------------+----+--------------+--------------------+-------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]|St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Emily Giffin|[[1212192000000]]|A woman’s happy m...|[24.95,]|[[1213488000000]]|St. Martin’s| [4]|           [3]|LOVE THE ONE YOU'...|          [3]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Emily Giffin|[[1212796800000]]|A woman’s happy m...|[24.95,]|[[1214092800000]]|St. Martin’s| [4]|           [4]|LOVE THE ONE YOU'...|          [4]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Emily Giffin|[[1213401600000]]|A woman’s happy m...|[24.95,]|[[1214697600000]]|St. Martin’s| [5]|           [4]|LOVE THE ONE YOU'...|          [5]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Emily Giffin|[[1214006400000]]|A woman’s happy m...|[24.95,]|[[1215302400000]]|St. Martin’s| [5]|           [5]|LOVE THE ONE YOU'...|          [6]|\n",
      "+--------------------+--------------------+------------+-----------------+--------------------+--------+-----------------+------------+----+--------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show isin operation\n",
    "dataframe[dataframe.author.isin(\"John Sandfor\", \"Emily Giffin\")].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:52.590074Z",
     "start_time": "2020-07-08T19:57:52.381628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|              author|               title|title LIKE % THE %|\n",
      "+--------------------+--------------------+------------------+\n",
      "|       Dean R Koontz|           ODD HOURS|             false|\n",
      "|     Stephenie Meyer|            THE HOST|             false|\n",
      "|        Emily Giffin|LOVE THE ONE YOU'...|              true|\n",
      "|   Patricia Cornwell|           THE FRONT|             false|\n",
      "|     Chuck Palahniuk|               SNUFF|             false|\n",
      "|James Patterson a...|SUNDAYS AT TIFFANY’S|             false|\n",
      "|       John Sandford|        PHANTOM PREY|             false|\n",
      "|       Jimmy Buffett|          SWINE NOT?|             false|\n",
      "|    Elizabeth George|     CARELESS IN RED|             false|\n",
      "|      David Baldacci|     THE WHOLE TRUTH|             false|\n",
      "|        Troy Denning|          INVINCIBLE|             false|\n",
      "|          James Frey|BRIGHT SHINY MORNING|             false|\n",
      "|         Garth Stein|THE ART OF RACING...|              true|\n",
      "|     Debbie Macomber|       TWENTY WISHES|             false|\n",
      "|         Jeff Shaara|      THE STEEL WAVE|             false|\n",
      "+--------------------+--------------------+------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show like operation\n",
    "dataframe.select(\"author\", \"title\", dataframe.title.like(\"% THE %\")).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:52.709752Z",
     "start_time": "2020-07-08T19:57:52.592066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------------------+\n",
      "|           author|               title|startswith(title, THE)|\n",
      "+-----------------+--------------------+----------------------+\n",
      "|    Dean R Koontz|           ODD HOURS|                 false|\n",
      "|  Stephenie Meyer|            THE HOST|                  true|\n",
      "|     Emily Giffin|LOVE THE ONE YOU'...|                 false|\n",
      "|Patricia Cornwell|           THE FRONT|                  true|\n",
      "|  Chuck Palahniuk|               SNUFF|                 false|\n",
      "+-----------------+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Startswith\n",
    "dataframe.select(\"author\", \"title\", dataframe.title.startswith(\"THE\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:52.838375Z",
     "start_time": "2020-07-08T19:57:52.712745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+\n",
      "|           author|               title|endswith(title, NT)|\n",
      "+-----------------+--------------------+-------------------+\n",
      "|    Dean R Koontz|           ODD HOURS|              false|\n",
      "|  Stephenie Meyer|            THE HOST|              false|\n",
      "|     Emily Giffin|LOVE THE ONE YOU'...|              false|\n",
      "|Patricia Cornwell|           THE FRONT|               true|\n",
      "|  Chuck Palahniuk|               SNUFF|              false|\n",
      "+-----------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Endswith\n",
    "dataframe.select(\"author\", \"title\", dataframe.title.endswith(\"NT\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:52.953579Z",
     "start_time": "2020-07-08T19:57:52.838375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     title|\n",
      "+----------+\n",
      "|ean R Koon|\n",
      "|tephenie M|\n",
      "|mily Giffi|\n",
      "|atricia Co|\n",
      "|huck Palah|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substring (start index, length)\n",
    "dataframe.select(dataframe.author.substr(2, 10).alias(\"title\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:53.057564Z",
     "start_time": "2020-07-08T19:57:52.953579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| title|\n",
      "+------+\n",
      "|an R K|\n",
      "|epheni|\n",
      "|ily Gi|\n",
      "|tricia|\n",
      "|uck Pa|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substring\n",
    "dataframe.select(dataframe.author.substr(3, 6).alias(\"title\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:53.189725Z",
     "start_time": "2020-07-08T19:57:53.057564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| title|\n",
      "+------+\n",
      "|Dean R|\n",
      "|Stephe|\n",
      "|Emily |\n",
      "|Patric|\n",
      "|Chuck |\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substring\n",
    "dataframe.select(dataframe.author.substr(1, 6).alias(\"title\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:53.288465Z",
     "start_time": "2020-07-08T19:57:53.191720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|title|\n",
      "+-----+\n",
      "|    D|\n",
      "|    S|\n",
      "|    E|\n",
      "|    P|\n",
      "|    C|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substring\n",
    "dataframe.select(dataframe.author.substr(0, 1).alias(\"title\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:53.316385Z",
     "start_time": "2020-07-08T19:57:53.291455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add new column\n",
    "dataframe = dataframe.withColumn(\"new_column\", lit(\"This is a new column\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:53.469978Z",
     "start_time": "2020-07-08T19:57:53.323368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|                 url|           author| bestsellers_date|         description|   price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]|[[1212883200000]]|       Bantam| [1]|           [0]|           ODD HOURS|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|Little, Brown| [2]|           [1]|            THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]| St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]|[[1212883200000]]|       Putnam| [4]|           [0]|           THE FRONT|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]|[[1212883200000]]|    Doubleday| [5]|           [0]|               SNUFF|          [1]|This is a new column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update column name\n",
    "dataframe = dataframe.withColumnRenamed('amazon_product_url', 'url')\n",
    "dataframe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:53.654487Z",
     "start_time": "2020-07-08T19:57:53.473966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|                 url|           author| bestsellers_date|         description|   price|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]| [1]|           [0]|           ODD HOURS|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]| [2]|           [1]|            THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]| [4]|           [0]|           THE FRONT|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]| [5]|           [0]|               SNUFF|          [1]|This is a new column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove column\n",
    "dataframe_remove = dataframe.drop(\"publisher\", \"published_date\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:53.769177Z",
     "start_time": "2020-07-08T19:57:53.656476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|                 url|           author| bestsellers_date|         description|   price|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]| [1]|           [0]|           ODD HOURS|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]| [2]|           [1]|            THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]| [4]|           [0]|           THE FRONT|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]| [5]|           [0]|               SNUFF|          [1]|This is a new column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove another way\n",
    "dataframe_remove_2 = dataframe.drop(dataframe.publisher).drop(dataframe.published_date).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:53.785134Z",
     "start_time": "2020-07-08T19:57:53.771174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'struct<$oid:string>'),\n",
       " ('url', 'string'),\n",
       " ('author', 'string'),\n",
       " ('bestsellers_date', 'struct<$date:struct<$numberLong:string>>'),\n",
       " ('description', 'string'),\n",
       " ('price', 'struct<$numberDouble:string,$numberInt:string>'),\n",
       " ('published_date', 'struct<$date:struct<$numberLong:string>>'),\n",
       " ('publisher', 'string'),\n",
       " ('rank', 'struct<$numberInt:string>'),\n",
       " ('rank_last_week', 'struct<$numberInt:string>'),\n",
       " ('title', 'string'),\n",
       " ('weeks_on_list', 'struct<$numberInt:string>'),\n",
       " ('new_column', 'string')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns and dtypes\n",
    "dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:53.916781Z",
     "start_time": "2020-07-08T19:57:53.788127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+--------------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|                 url|              author| bestsellers_date|         description|   price|   published_date|           publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+--------------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]|[[1212883200000]]|              Bantam| [1]|           [0]|           ODD HOURS|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|       Little, Brown| [2]|           [1]|            THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]|        St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]|[[1212883200000]]|              Putnam| [4]|           [0]|           THE FRONT|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]|[[1212883200000]]|           Doubleday| [5]|           [0]|               SNUFF|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|[[1211587200000]]|A woman finds an ...|[24.99,]|[[1212883200000]]|       Little, Brown| [6]|           [3]|SUNDAYS AT TIFFANY’S|          [4]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       John Sandford|[[1211587200000]]|The Minneapolis d...|[26.95,]|[[1212883200000]]|              Putnam| [7]|           [4]|        PHANTOM PREY|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|[[1211587200000]]|A Southern family...|[21.99,]|[[1212883200000]]|       Little, Brown| [8]|           [6]|          SWINE NOT?|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|[[1211587200000]]|In Cornwall, tryi...|[27.95,]|[[1212883200000]]|              Harper| [9]|           [8]|     CARELESS IN RED|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|[[1211587200000]]|An intelligence a...|[26.99,]|[[1212883200000]]|       Grand Central|[10]|           [7]|     THE WHOLE TRUTH|          [5]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|        Troy Denning|[[1211587200000]]|The New Jedi orde...|  [, 27]|[[1212883200000]]|  Del Rey/Ballantine|[11]|           [5]|          INVINCIBLE|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|          James Frey|[[1211587200000]]|A novel, set in L...|[26.95,]|[[1212883200000]]|              Harper|[12]|           [9]|BRIGHT SHINY MORNING|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|         Garth Stein|[[1211587200000]]|A Lab-terrier mix...|[23.95,]|[[1212883200000]]|              Harper|[13]|           [0]|THE ART OF RACING...|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Debbie Macomber|[[1211587200000]]|A widow who owns ...|[24.95,]|[[1212883200000]]|                Mira|[14]|          [10]|       TWENTY WISHES|          [4]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|         Jeff Shaara|[[1211587200000]]|A novel about the...|  [, 28]|[[1212883200000]]|          Ballantine|[15]|          [11]|      THE STEEL WAVE|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Phillip Margolin|[[1211587200000]]|                    |   [, 0]|[[1212883200000]]|HarperCollins Pub...|[16]|           [0]| EXECUTIVE PRIVILEGE|          [0]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       Jhumpa Lahiri|[[1211587200000]]|Stories of the an...|   [, 0]|[[1212883200000]]|               Knopf|[17]|           [0]|  UNACCUSTOMED EARTH|          [0]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|      Joseph O'Neill|[[1211587200000]]|A Dutchman desert...|   [, 0]|[[1212883200000]]|Knopf Publishing ...|[18]|           [0]|          NETHERLAND|          [0]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|        John Grisham|[[1211587200000]]|Political and leg...|   [, 0]|[[1212883200000]]|Doubleday Publishing|[19]|           [0]|          THE APPEAL|          [0]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       James Rollins|[[1211587200000]]|                    |   [, 0]|[[1212883200000]]|Random House Publ...|[20]|           [0]|INDIANA JONES AND...|          [0]|This is a new column|\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+--------------------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contents\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:54.044443Z",
     "start_time": "2020-07-08T19:57:53.918775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_id=Row($oid='5b4aa4ead3089013507db18b'), url='http://www.amazon.com/Odd-Hours-Dean-Koontz/dp/0553807056?tag=NYTBS-20', author='Dean R Koontz', bestsellers_date=Row($date=Row($numberLong='1211587200000')), description='Odd Thomas, who can communicate with the dead, confronts evil forces in a California coastal town.', price=Row($numberDouble=None, $numberInt='27'), published_date=Row($date=Row($numberLong='1212883200000')), publisher='Bantam', rank=Row($numberInt='1'), rank_last_week=Row($numberInt='0'), title='ODD HOURS', weeks_on_list=Row($numberInt='1'), new_column='This is a new column')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First n rows\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:54.135198Z",
     "start_time": "2020-07-08T19:57:54.047432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_id=Row($oid='5b4aa4ead3089013507db18b'), url='http://www.amazon.com/Odd-Hours-Dean-Koontz/dp/0553807056?tag=NYTBS-20', author='Dean R Koontz', bestsellers_date=Row($date=Row($numberLong='1211587200000')), description='Odd Thomas, who can communicate with the dead, confronts evil forces in a California coastal town.', price=Row($numberDouble=None, $numberInt='27'), published_date=Row($date=Row($numberLong='1212883200000')), publisher='Bantam', rank=Row($numberInt='1'), rank_last_week=Row($numberInt='0'), title='ODD HOURS', weeks_on_list=Row($numberInt='1'), new_column='This is a new column')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First row\n",
    "dataframe.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:55.249219Z",
     "start_time": "2020-07-08T19:57:54.138191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------+--------------------+---------+------------------+--------------------+\n",
      "|summary|                 url|         author|         description|publisher|             title|          new_column|\n",
      "+-------+--------------------+---------------+--------------------+---------+------------------+--------------------+\n",
      "|  count|               10195|          10195|               10195|    10195|             10195|               10195|\n",
      "|   mean|                null|           null|                null|     null|1877.7142857142858|                null|\n",
      "| stddev|                null|           null|                null|     null| 370.9760613506458|                null|\n",
      "|    min|http://www.amazon...|        AJ Finn|                    |      ACE|  10TH ANNIVERSARY|This is a new column|\n",
      "|    max|https://www.amazo...|various authors|’Tis for the Rebe...|allantine|               ZOO|This is a new column|\n",
      "+-------+--------------------+---------------+--------------------+---------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "dataframe.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:55.258196Z",
     "start_time": "2020-07-08T19:57:55.251213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id',\n",
       " 'url',\n",
       " 'author',\n",
       " 'bestsellers_date',\n",
       " 'description',\n",
       " 'price',\n",
       " 'published_date',\n",
       " 'publisher',\n",
       " 'rank',\n",
       " 'rank_last_week',\n",
       " 'title',\n",
       " 'weeks_on_list',\n",
       " 'new_column']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns\n",
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:55.461651Z",
     "start_time": "2020-07-08T19:57:55.261188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10195"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows\n",
    "dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:56.338309Z",
     "start_time": "2020-07-08T19:57:55.463646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10195"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of distinct rows\n",
    "dataframe.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:56.367231Z",
     "start_time": "2020-07-08T19:57:56.341302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [_id#14, amazon_product_url#15 AS url#369, author#16, bestsellers_date#17, description#18, price#19, published_date#20, publisher#21, rank#22, rank_last_week#23, title#24, weeks_on_list#25, This is a new column AS new_column#355]\n",
      "+- FileScan json [_id#14,amazon_product_url#15,author#16,bestsellers_date#17,description#18,price#19,published_date#20,publisher#21,rank#22,rank_last_week#23,title#24,weeks_on_list#25] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[file:/C:/Spark/02-raw-data/nyt2.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_id:struct<$oid:string>,amazon_product_url:string,author:string,bestsellers_date:struct<$d...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Physical plan\n",
    "dataframe.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:56.776138Z",
     "start_time": "2020-07-08T19:57:56.369226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              author|count|\n",
      "+--------------------+-----+\n",
      "|          James Frey|    2|\n",
      "|    Elin Hilderbrand|   58|\n",
      "|   Sharon Kay Penman|    2|\n",
      "|         Kate Jacobs|    3|\n",
      "|       Karen Robards|    6|\n",
      "|     Gary Shteyngart|    3|\n",
      "|         Lisa Genova|    7|\n",
      "|James Patterson a...|   30|\n",
      "|         Ruth Reichl|    3|\n",
      "|         JRR Tolkien|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by\n",
    "dataframe.groupBy(\"author\").count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:56.954661Z",
     "start_time": "2020-07-08T19:57:56.779130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------+-------------+--------------------+\n",
      "|                 _id|                 url|         author| bestsellers_date|         description|   price|   published_date|    publisher|rank|rank_last_week|   title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|Little, Brown| [2]|           [1]|THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1212192000000]]|Aliens have taken...|[25.99,]|[[1213488000000]]|Little, Brown| [2]|           [2]|THE HOST|          [4]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1212796800000]]|Aliens have taken...|[25.99,]|[[1214092800000]]|Little, Brown| [2]|           [2]|THE HOST|          [5]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1213401600000]]|Aliens have taken...|[25.99,]|[[1214697600000]]|Little, Brown| [3]|           [2]|THE HOST|          [6]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1214006400000]]|Aliens have taken...|[25.99,]|[[1215302400000]]|Little, Brown| [3]|           [3]|THE HOST|          [7]|This is a new column|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter\n",
    "dataframe.filter(dataframe[\"title\"] == 'THE HOST').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:57.048412Z",
     "start_time": "2020-07-08T19:57:56.957654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe with 10 partitions\n",
    "dataframe.repartition(10).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:57.151139Z",
     "start_time": "2020-07-08T19:57:57.051403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe with 1 partition\n",
    "dataframe.coalesce(1).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:57.331654Z",
     "start_time": "2020-07-08T19:57:57.154129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|                 url|         author| bestsellers_date|         description|   price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]|[[1212883200000]]|       Bantam| [1]|           [0]|           ODD HOURS|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|Little, Brown| [2]|           [1]|            THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|   Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]| St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|This is a new column|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registering a table\n",
    "dataframe.registerTempTable(\"df\")\n",
    "sc.sql(\"select * from df\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:57.369551Z",
     "start_time": "2020-07-08T19:57:57.334644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting dataframe into an RDD\n",
    "rdd_convert = dataframe.rdd\n",
    "type(rdd_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:57.380523Z",
     "start_time": "2020-07-08T19:57:57.372546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:57.410443Z",
     "start_time": "2020-07-08T19:57:57.383514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RDD in module pyspark.rdd object:\n",
      "\n",
      "class RDD(builtins.object)\n",
      " |  RDD(jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
      " |  \n",
      " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      " |  Represents an immutable, partitioned collection of elements that can be\n",
      " |  operated on in parallel.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> (rdd + rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  aggregate(self, zeroValue, seqOp, combOp)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given combine functions and a neutral \"zero\n",
      " |      value.\"\n",
      " |      \n",
      " |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify ``t2``.\n",
      " |      \n",
      " |      The first function (seqOp) can return a different result type, U, than\n",
      " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
      " |      an U and one operation for merging two U\n",
      " |      \n",
      " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (10, 4)\n",
      " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (0, 0)\n",
      " |  \n",
      " |  aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash at 0x000001F8BED84160>)\n",
      " |      Aggregate the values of each key, using given combine functions and a neutral\n",
      " |      \"zero value\". This function can return a different result type, U, than the type\n",
      " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
      " |      a U and one operation for merging two U's, The former operation is used for merging\n",
      " |      values within a partition, and the latter is used for merging values between\n",
      " |      partitions. To avoid memory allocation, both of these functions are\n",
      " |      allowed to modify and return their first argument instead of creating a new U.\n",
      " |  \n",
      " |  barrier(self)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n",
      " |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n",
      " |      entire stage and relaunch all tasks for this stage.\n",
      " |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n",
      " |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n",
      " |      \n",
      " |      :return: an :class:`RDDBarrier` instance that provides actions within a barrier stage.\n",
      " |      \n",
      " |      .. seealso:: :class:`BarrierTaskContext`\n",
      " |      .. seealso:: `SPIP: Barrier Execution Mode\n",
      " |          <http://jira.apache.org/jira/browse/SPARK-24374>`_\n",
      " |      .. seealso:: `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
      " |  \n",
      " |  cartesian(self, other)\n",
      " |      Return the Cartesian product of this RDD and another one, that is, the\n",
      " |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n",
      " |      ``b`` is in `other`.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
      " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      " |  \n",
      " |  checkpoint(self)\n",
      " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
      " |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n",
      " |      all references to its parent RDDs will be removed. This function must\n",
      " |      be called before any job has been executed on this RDD. It is strongly\n",
      " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
      " |      on a file will require recomputation.\n",
      " |  \n",
      " |  coalesce(self, numPartitions, shuffle=False)\n",
      " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
      " |      [[1], [2, 3], [4, 5]]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
      " |      [[1, 2, 3, 4, 5]]\n",
      " |  \n",
      " |  cogroup(self, other, numPartitions=None)\n",
      " |      For each key k in `self` or `other`, return a resulting RDD that\n",
      " |      contains a tuple with the list of values for that key in `self` as\n",
      " |      well as `other`.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
      " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Return a list that contains all of the elements in this RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |  \n",
      " |  collectAsMap(self)\n",
      " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting data is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
      " |      >>> m[1]\n",
      " |      2\n",
      " |      >>> m[3]\n",
      " |      4\n",
      " |  \n",
      " |  collectWithJobGroup(self, groupId, description, interruptOnCancel=False)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      When collect rdd, use this method to specify job group.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash at 0x000001F8BED84160>)\n",
      " |      Generic function to combine the elements for each key using a custom\n",
      " |      set of aggregation functions.\n",
      " |      \n",
      " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
      " |      type\" C.\n",
      " |      \n",
      " |      Users provide three functions:\n",
      " |      \n",
      " |          - `createCombiner`, which turns a V into a C (e.g., creates\n",
      " |            a one-element list)\n",
      " |          - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n",
      " |            a list)\n",
      " |          - `mergeCombiners`, to combine two C's into a single one (e.g., merges\n",
      " |            the lists)\n",
      " |      \n",
      " |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
      " |      modify and return their first argument instead of creating a new C.\n",
      " |      \n",
      " |      In addition, users can control the partitioning of the output RDD.\n",
      " |      \n",
      " |      .. note:: V and C can be different -- for example, one might group an RDD of type\n",
      " |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      " |      >>> def to_list(a):\n",
      " |      ...     return [a]\n",
      " |      ...\n",
      " |      >>> def append(a, b):\n",
      " |      ...     a.append(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> def extend(a, b):\n",
      " |      ...     a.extend(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> sorted(x.combineByKey(to_list, append, extend).collect())\n",
      " |      [('a', [1, 2]), ('b', [1])]\n",
      " |  \n",
      " |  count(self)\n",
      " |      Return the number of elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).count()\n",
      " |      3\n",
      " |  \n",
      " |  countApprox(self, timeout, confidence=0.95)\n",
      " |      Approximate version of count() that returns a potentially incomplete\n",
      " |      result within a timeout, even if not all tasks have finished.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> rdd.countApprox(1000, 1.0)\n",
      " |      1000\n",
      " |  \n",
      " |  countApproxDistinct(self, relativeSD=0.05)\n",
      " |      Return approximate number of distinct elements in the RDD.\n",
      " |      \n",
      " |      The algorithm used is based on streamlib's implementation of\n",
      " |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
      " |      of The Art Cardinality Estimation Algorithm\", available here\n",
      " |      <https://doi.org/10.1145/2452376.2452456>`_.\n",
      " |      \n",
      " |      :param relativeSD: Relative accuracy. Smaller values create\n",
      " |                         counters that require more space.\n",
      " |                         It must be greater than 0.000017.\n",
      " |      \n",
      " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
      " |      >>> 900 < n < 1100\n",
      " |      True\n",
      " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
      " |      >>> 16 < n < 24\n",
      " |      True\n",
      " |  \n",
      " |  countByKey(self)\n",
      " |      Count the number of elements for each key, and return the result to the\n",
      " |      master as a dictionary.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.countByKey().items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  countByValue(self)\n",
      " |      Return the count of each unique value in this RDD as a dictionary of\n",
      " |      (value, count) pairs.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
      " |      [(1, 2), (2, 3)]\n",
      " |  \n",
      " |  distinct(self, numPartitions=None)\n",
      " |      Return a new RDD containing the distinct elements in this RDD.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  filter(self, f)\n",
      " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
      " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  first(self)\n",
      " |      Return the first element in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).first()\n",
      " |      2\n",
      " |      >>> sc.parallelize([]).first()\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: RDD is empty\n",
      " |  \n",
      " |  flatMap(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by first applying a function to all elements of this\n",
      " |      RDD, and then flattening the results.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
      " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      " |      [1, 1, 1, 2, 2, 3]\n",
      " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      " |  \n",
      " |  flatMapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a flatMap function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
      " |      >>> def f(x): return x\n",
      " |      >>> x.flatMapValues(f).collect()\n",
      " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      " |  \n",
      " |  fold(self, zeroValue, op)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
      " |      \n",
      " |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify ``t2``.\n",
      " |      \n",
      " |      This behaves somewhat differently from fold operations implemented\n",
      " |      for non-distributed collections in functional languages like Scala.\n",
      " |      This fold operation may be applied to partitions individually, and then\n",
      " |      fold those results into the final result, rather than apply the fold\n",
      " |      to each element sequentially in some defined ordering. For functions\n",
      " |      that are not commutative, the result may differ from that of a fold\n",
      " |      applied to a non-distributed collection.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
      " |      15\n",
      " |  \n",
      " |  foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash at 0x000001F8BED84160>)\n",
      " |      Merge the values for each key using an associative function \"func\"\n",
      " |      and a neutral \"zeroValue\" which may be added to the result an\n",
      " |      arbitrary number of times, and must not change the result\n",
      " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> from operator import add\n",
      " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies a function to all elements of this RDD.\n",
      " |      \n",
      " |      >>> def f(x): print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> def f(iterator):\n",
      " |      ...     for x in iterator:\n",
      " |      ...          print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
      " |  \n",
      " |  fullOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, v) in `self`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      " |      (k, (v, None)) if no elements in `other` have key k.\n",
      " |      \n",
      " |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n",
      " |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n",
      " |      (k, (None, w)) if no elements in `self` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
      " |      >>> sorted(x.fullOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
      " |  \n",
      " |  getCheckpointFile(self)\n",
      " |      Gets the name of the file to which this RDD was checkpointed\n",
      " |      \n",
      " |      Not defined if RDD is checkpointed locally.\n",
      " |  \n",
      " |  getNumPartitions(self)\n",
      " |      Returns the number of partitions in RDD\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> rdd.getNumPartitions()\n",
      " |      2\n",
      " |  \n",
      " |  getStorageLevel(self)\n",
      " |      Get the RDD's current storage level.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1,2])\n",
      " |      >>> rdd1.getStorageLevel()\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> print(rdd1.getStorageLevel())\n",
      " |      Serialized 1x Replicated\n",
      " |  \n",
      " |  glom(self)\n",
      " |      Return an RDD created by coalescing all elements within each partition\n",
      " |      into a list.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1, 2], [3, 4]]\n",
      " |  \n",
      " |  groupBy(self, f, numPartitions=None, partitionFunc=<function portable_hash at 0x000001F8BED84160>)\n",
      " |      Return an RDD of grouped items.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
      " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
      " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
      " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
      " |  \n",
      " |  groupByKey(self, numPartitions=None, partitionFunc=<function portable_hash at 0x000001F8BED84160>)\n",
      " |      Group the values for each key in the RDD into a single sequence.\n",
      " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
      " |      \n",
      " |      .. note:: If you are grouping in order to perform an aggregation (such as a\n",
      " |          sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      " |          provide much better performance.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      " |      [('a', [1, 1]), ('b', [1])]\n",
      " |  \n",
      " |  groupWith(self, other, *others)\n",
      " |      Alias for cogroup but with support for multiple RDDs.\n",
      " |      \n",
      " |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> z = sc.parallelize([(\"b\", 42)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
      " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
      " |  \n",
      " |  histogram(self, buckets)\n",
      " |      Compute a histogram using the provided buckets. The buckets\n",
      " |      are all open to the right except for the last which is closed.\n",
      " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
      " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
      " |      and 50 we would have a histogram of 1,0,1.\n",
      " |      \n",
      " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
      " |      this can be switched from an O(log n) inseration to O(1) per\n",
      " |      element (where n is the number of buckets).\n",
      " |      \n",
      " |      Buckets must be sorted, not contain any duplicates, and have\n",
      " |      at least two elements.\n",
      " |      \n",
      " |      If `buckets` is a number, it will generate buckets which are\n",
      " |      evenly spaced between the minimum and maximum of the RDD. For\n",
      " |      example, if the min value is 0 and the max is 100, given `buckets`\n",
      " |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
      " |      be at least 1. An exception is raised if the RDD contains infinity.\n",
      " |      If the elements in the RDD do not vary (max == min), a single bucket\n",
      " |      will be used.\n",
      " |      \n",
      " |      The return value is a tuple of buckets and histogram.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(51))\n",
      " |      >>> rdd.histogram(2)\n",
      " |      ([0, 25, 50], [25, 26])\n",
      " |      >>> rdd.histogram([0, 5, 25, 50])\n",
      " |      ([0, 5, 25, 50], [5, 20, 26])\n",
      " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
      " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
      " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
      " |      (('a', 'b', 'c'), [2, 2])\n",
      " |  \n",
      " |  id(self)\n",
      " |      A unique ID for this RDD (within its SparkContext).\n",
      " |  \n",
      " |  intersection(self, other)\n",
      " |      Return the intersection of this RDD and another one. The output will\n",
      " |      not contain any duplicate elements, even if the input RDDs did.\n",
      " |      \n",
      " |      .. note:: This method performs a shuffle internally.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
      " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
      " |      >>> rdd1.intersection(rdd2).collect()\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  isCheckpointed(self)\n",
      " |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
      " |  \n",
      " |  isEmpty(self)\n",
      " |      Returns true if and only if the RDD contains no elements at all.\n",
      " |      \n",
      " |      .. note:: an RDD may be empty even when it has at least 1 partition.\n",
      " |      \n",
      " |      >>> sc.parallelize([]).isEmpty()\n",
      " |      True\n",
      " |      >>> sc.parallelize([1]).isEmpty()\n",
      " |      False\n",
      " |  \n",
      " |  isLocallyCheckpointed(self)\n",
      " |      Return whether this RDD is marked for local checkpointing.\n",
      " |      \n",
      " |      Exposed for testing.\n",
      " |  \n",
      " |  join(self, other, numPartitions=None)\n",
      " |      Return an RDD containing all pairs of elements with matching keys in\n",
      " |      `self` and `other`.\n",
      " |      \n",
      " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      " |      (k, v1) is in `self` and (k, v2) is in `other`.\n",
      " |      \n",
      " |      Performs a hash join across the cluster.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      " |      >>> sorted(x.join(y).collect())\n",
      " |      [('a', (1, 2)), ('a', (1, 3))]\n",
      " |  \n",
      " |  keyBy(self, f)\n",
      " |      Creates tuples of the elements in this RDD by applying `f`.\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
      " |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
      " |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
      " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Return an RDD with the keys of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
      " |      >>> m.collect()\n",
      " |      [1, 3]\n",
      " |  \n",
      " |  leftOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a left outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, v) in `self`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      " |      (k, (v, None)) if no elements in `other` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(x.leftOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None))]\n",
      " |  \n",
      " |  localCheckpoint(self)\n",
      " |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
      " |      \n",
      " |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
      " |      step of replicating the materialized data in a reliable distributed file system. This is\n",
      " |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
      " |      \n",
      " |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
      " |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
      " |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
      " |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
      " |      \n",
      " |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
      " |      with their cached blocks. If you must use both features, you are advised to set\n",
      " |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n",
      " |      \n",
      " |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n",
      " |  \n",
      " |  lookup(self, key)\n",
      " |      Return the list of values in the RDD for key `key`. This operation\n",
      " |      is done efficiently if the RDD has a known partitioner by only\n",
      " |      searching the partition that the key maps to.\n",
      " |      \n",
      " |      >>> l = range(1000)\n",
      " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
      " |      >>> rdd.lookup(42)  # slow\n",
      " |      [42]\n",
      " |      >>> sorted = rdd.sortByKey()\n",
      " |      >>> sorted.lookup(42)  # fast\n",
      " |      [42]\n",
      " |      >>> sorted.lookup(1024)\n",
      " |      []\n",
      " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
      " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
      " |      ['c']\n",
      " |  \n",
      " |  map(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each element of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      " |      [('a', 1), ('b', 1), ('c', 1)]\n",
      " |  \n",
      " |  mapPartitions(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> def f(iterator): yield sum(iterator)\n",
      " |      >>> rdd.mapPartitions(f).collect()\n",
      " |      [3, 7]\n",
      " |  \n",
      " |  mapPartitionsWithIndex(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapPartitionsWithSplit(self, f, preservesPartitioning=False)\n",
      " |      Deprecated: use mapPartitionsWithIndex instead.\n",
      " |      \n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a map function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
      " |      >>> def f(x): return len(x)\n",
      " |      >>> x.mapValues(f).collect()\n",
      " |      [('a', 3), ('b', 1)]\n",
      " |  \n",
      " |  max(self, key=None)\n",
      " |      Find the maximum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.max()\n",
      " |      43.0\n",
      " |      >>> rdd.max(key=str)\n",
      " |      5.0\n",
      " |  \n",
      " |  mean(self)\n",
      " |      Compute the mean of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
      " |      2.0\n",
      " |  \n",
      " |  meanApprox(self, timeout, confidence=0.95)\n",
      " |      Approximate operation to return the mean within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000)) / 1000.0\n",
      " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  min(self, key=None)\n",
      " |      Find the minimum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.min()\n",
      " |      2.0\n",
      " |      >>> rdd.min(key=str)\n",
      " |      10.0\n",
      " |  \n",
      " |  name(self)\n",
      " |      Return the name of this RDD.\n",
      " |  \n",
      " |  partitionBy(self, numPartitions, partitionFunc=<function portable_hash at 0x000001F8BED84160>)\n",
      " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
      " |      \n",
      " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
      " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
      " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
      " |      0\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(False, True, False, False, 1))\n",
      " |      Set this RDD's storage level to persist its values across operations\n",
      " |      after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the RDD does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_ONLY`).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> rdd.persist().is_cached\n",
      " |      True\n",
      " |  \n",
      " |  pipe(self, command, env=None, checkCode=False)\n",
      " |      Return an RDD created by piping elements to a forked external process.\n",
      " |      \n",
      " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
      " |      ['1', '2', '', '3']\n",
      " |      \n",
      " |      :param checkCode: whether or not to check the return value of the shell command.\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this RDD with the provided weights.\n",
      " |      \n",
      " |      :param weights: weights for splits, will be normalized if they don't sum to 1\n",
      " |      :param seed: random seed\n",
      " |      :return: split RDDs in a list\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(500), 1)\n",
      " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      " |      >>> len(rdd1.collect() + rdd2.collect())\n",
      " |      500\n",
      " |      >>> 150 < rdd1.count() < 250\n",
      " |      True\n",
      " |      >>> 250 < rdd2.count() < 350\n",
      " |      True\n",
      " |  \n",
      " |  reduce(self, f)\n",
      " |      Reduces the elements of this RDD using the specified commutative and\n",
      " |      associative binary operator. Currently reduces partitions locally.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      " |      15\n",
      " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      " |      10\n",
      " |      >>> sc.parallelize([]).reduce(add)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Can not reduce() empty RDD\n",
      " |  \n",
      " |  reduceByKey(self, func, numPartitions=None, partitionFunc=<function portable_hash at 0x000001F8BED84160>)\n",
      " |      Merge the values for each key using an associative and commutative reduce function.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      Output will be partitioned with `numPartitions` partitions, or\n",
      " |      the default parallelism level if `numPartitions` is not specified.\n",
      " |      Default partitioner is hash-partition.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  reduceByKeyLocally(self, func)\n",
      " |      Merge the values for each key using an associative and commutative reduce function, but\n",
      " |      return the results immediately to the master as a dictionary.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  repartition(self, numPartitions)\n",
      " |      Return a new RDD that has exactly numPartitions partitions.\n",
      " |      \n",
      " |      Can increase or decrease the level of parallelism in this RDD.\n",
      " |      Internally, this uses a shuffle to redistribute data.\n",
      " |      If you are decreasing the number of partitions in this RDD, consider\n",
      " |      using `coalesce`, which can avoid performing a shuffle.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1], [2, 3], [4, 5], [6, 7]]\n",
      " |      >>> len(rdd.repartition(2).glom().collect())\n",
      " |      2\n",
      " |      >>> len(rdd.repartition(10).glom().collect())\n",
      " |      10\n",
      " |  \n",
      " |  repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=<function portable_hash at 0x000001F8BED84160>, ascending=True, keyfunc=<function RDD.<lambda> at 0x000001F8C01783A0>)\n",
      " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
      " |      sort records by their keys.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
      " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
      " |      >>> rdd2.glom().collect()\n",
      " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
      " |  \n",
      " |  rightOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, w) in `other`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
      " |      if no elements in `self` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(y.rightOuterJoin(x).collect())\n",
      " |      [('a', (2, 1)), ('b', (None, 4))]\n",
      " |  \n",
      " |  sample(self, withReplacement, fraction, seed=None)\n",
      " |      Return a sampled subset of this RDD.\n",
      " |      \n",
      " |      :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n",
      " |      :param fraction: expected size of the sample as a fraction of this RDD's size\n",
      " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
      " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
      " |      :param seed: seed for the random number generator\n",
      " |      \n",
      " |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |          count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(100), 4)\n",
      " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
      " |      True\n",
      " |  \n",
      " |  sampleByKey(self, withReplacement, fractions, seed=None)\n",
      " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
      " |      Create a sample of this RDD using variable sampling rates for\n",
      " |      different keys as specified by fractions, a key to sampling rate map.\n",
      " |      \n",
      " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
      " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
      " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
      " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
      " |      True\n",
      " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
      " |      True\n",
      " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
      " |      True\n",
      " |  \n",
      " |  sampleStdev(self)\n",
      " |      Compute the sample standard deviation of this RDD's elements (which\n",
      " |      corrects for bias in estimating the standard deviation by dividing by\n",
      " |      N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
      " |      1.0\n",
      " |  \n",
      " |  sampleVariance(self)\n",
      " |      Compute the sample variance of this RDD's elements (which corrects\n",
      " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
      " |      1.0\n",
      " |  \n",
      " |  saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: (None by default)\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop job configuration, passed in as a dict (None by default)\n",
      " |  \n",
      " |  saveAsPickleFile(self, path, batchSize=10)\n",
      " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
      " |      used is :class:`pyspark.serializers.PickleSerializer`, default batch size\n",
      " |      is 10.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
      " |      ['1', '2', 'rdd', 'spark']\n",
      " |  \n",
      " |  saveAsSequenceFile(self, path, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n",
      " |      RDD's key and value types. The mechanism is as follows:\n",
      " |      \n",
      " |          1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
      " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
      " |      \n",
      " |      :param path: path to sequence file\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsTextFile(self, path, compressionCodecClass=None)\n",
      " |      Save this RDD as a text file, using string representations of elements.\n",
      " |      \n",
      " |      :param path: path to text file\n",
      " |      :param compressionCodecClass: (None by default) string i.e.\n",
      " |          \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      \n",
      " |      >>> tempFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
      " |      >>> from fileinput import input\n",
      " |      >>> from glob import glob\n",
      " |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
      " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      " |      \n",
      " |      Empty lines are tolerated when saving to text files.\n",
      " |      \n",
      " |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile2.close()\n",
      " |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
      " |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
      " |      '\\n\\n\\nbar\\nfoo\\n'\n",
      " |      \n",
      " |      Using compressionCodecClass\n",
      " |      \n",
      " |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile3.close()\n",
      " |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
      " |      >>> from fileinput import input, hook_compressed\n",
      " |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
      " |      >>> b''.join(result).decode('utf-8')\n",
      " |      'bar\\nfoo\\n'\n",
      " |  \n",
      " |  setName(self, name)\n",
      " |      Assign a name to this RDD.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 2])\n",
      " |      >>> rdd1.setName('RDD1').name()\n",
      " |      'RDD1'\n",
      " |  \n",
      " |  sortBy(self, keyfunc, ascending=True, numPartitions=None)\n",
      " |      Sorts this RDD by the given keyfunc\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |  \n",
      " |  sortByKey(self, ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda> at 0x000001F8C01784C0>)\n",
      " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
      " |      ('1', 3)\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
      " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
      " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
      " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
      " |  \n",
      " |  stats(self)\n",
      " |      Return a :class:`StatCounter` object that captures the mean, variance\n",
      " |      and count of the RDD's elements in one operation.\n",
      " |  \n",
      " |  stdev(self)\n",
      " |      Compute the standard deviation of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
      " |      0.816...\n",
      " |  \n",
      " |  subtract(self, other, numPartitions=None)\n",
      " |      Return each value in `self` that is not contained in `other`.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtract(y).collect())\n",
      " |      [('a', 1), ('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  subtractByKey(self, other, numPartitions=None)\n",
      " |      Return each (key, value) pair in `self` that has no pair with matching\n",
      " |      key in `other`.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtractByKey(y).collect())\n",
      " |      [('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  sum(self)\n",
      " |      Add up the elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
      " |      6.0\n",
      " |  \n",
      " |  sumApprox(self, timeout, confidence=0.95)\n",
      " |      Approximate operation to return the sum within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000))\n",
      " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Take the first num elements of the RDD.\n",
      " |      \n",
      " |      It works by first scanning one partition, and use the results from\n",
      " |      that partition to estimate the number of additional partitions needed\n",
      " |      to satisfy the limit.\n",
      " |      \n",
      " |      Translated from the Scala implementation in RDD#take().\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      " |      [2, 3]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      " |      [91, 92, 93]\n",
      " |  \n",
      " |  takeOrdered(self, num, key=None)\n",
      " |      Get the N elements from an RDD ordered in ascending order or as\n",
      " |      specified by the optional key function.\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
      " |      [1, 2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
      " |      [10, 9, 7, 6, 5, 4]\n",
      " |  \n",
      " |  takeSample(self, withReplacement, num, seed=None)\n",
      " |      Return a fixed-size sampled subset of this RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(0, 10))\n",
      " |      >>> len(rdd.takeSample(True, 20, 1))\n",
      " |      20\n",
      " |      >>> len(rdd.takeSample(False, 5, 2))\n",
      " |      5\n",
      " |      >>> len(rdd.takeSample(False, 15, 3))\n",
      " |      10\n",
      " |  \n",
      " |  toDF(self, schema=None, sampleRatio=None)\n",
      " |      Converts current :class:`RDD` into a :class:`DataFrame`\n",
      " |      \n",
      " |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n",
      " |      \n",
      " |      :param schema: a :class:`pyspark.sql.types.StructType` or list of names of columns\n",
      " |      :param samplingRatio: the sample ratio of rows used for inferring\n",
      " |      :return: a DataFrame\n",
      " |      \n",
      " |      >>> rdd.toDF().collect()\n",
      " |      [Row(name=u'Alice', age=1)]\n",
      " |  \n",
      " |  toDebugString(self)\n",
      " |      A description of this RDD and its recursive dependencies for debugging.\n",
      " |  \n",
      " |  toLocalIterator(self, prefetchPartitions=False)\n",
      " |      Return an iterator that contains all of the elements in this RDD.\n",
      " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
      " |      With prefetch it may consume up to the memory of the 2 largest partitions.\n",
      " |      \n",
      " |      :param prefetchPartitions: If Spark should pre-fetch the next partition\n",
      " |                                 before it is needed.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(10))\n",
      " |      >>> [x for x in rdd.toLocalIterator()]\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  top(self, num, key=None)\n",
      " |      Get the top N elements from an RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      .. note:: It returns the list sorted in descending order.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      " |      [12]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      " |      [6, 5]\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      " |      [4, 3, 2]\n",
      " |  \n",
      " |  treeAggregate(self, zeroValue, seqOp, combOp, depth=2)\n",
      " |      Aggregates the elements of this RDD in a multi-level tree\n",
      " |      pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeAggregate(0, add, add)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  treeReduce(self, f, depth=2)\n",
      " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeReduce(add)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> rdd.union(rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  unpersist(self, blocking=False)\n",
      " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. versionchanged:: 3.0.0\n",
      " |         Added optional argument `blocking` to specify whether to block until all\n",
      " |         blocks are deleted.\n",
      " |  \n",
      " |  values(self)\n",
      " |      Return an RDD with the values of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
      " |      >>> m.collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  variance(self)\n",
      " |      Compute the variance of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
      " |      0.666...\n",
      " |  \n",
      " |  zip(self, other)\n",
      " |      Zips this RDD with another one, returning key-value pairs with the\n",
      " |      first element in each RDD second element in each RDD, etc. Assumes\n",
      " |      that the two RDDs have the same number of partitions and the same\n",
      " |      number of elements in each partition (e.g. one was made through\n",
      " |      a map on the other).\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,5))\n",
      " |      >>> y = sc.parallelize(range(1000, 1005))\n",
      " |      >>> x.zip(y).collect()\n",
      " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
      " |  \n",
      " |  zipWithIndex(self)\n",
      " |      Zips this RDD with its element indices.\n",
      " |      \n",
      " |      The ordering is first based on the partition index and then the\n",
      " |      ordering of items within each partition. So the first item in\n",
      " |      the first partition gets index 0, and the last item in the last\n",
      " |      partition receives the largest index.\n",
      " |      \n",
      " |      This method needs to trigger a spark job when this RDD contains\n",
      " |      more than one partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
      " |  \n",
      " |  zipWithUniqueId(self)\n",
      " |      Zips this RDD with generated unique Long ids.\n",
      " |      \n",
      " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
      " |      n is the number of partitions. So there may exist gaps, but this\n",
      " |      method won't trigger a spark job, which is different from\n",
      " |      :meth:`zipWithIndex`.\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  context\n",
      " |      The :class:`SparkContext` that this RDD was created on.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rdd_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:58.688029Z",
     "start_time": "2020-07-08T19:57:57.413435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"_id\":{\"$oid\":\"5b4aa4ead3089013507db18b\"},\"url\":\"http://www.amazon.com/Odd-Hours-Dean-Koontz/dp/0553807056?tag=NYTBS-20\",\"author\":\"Dean R Koontz\",\"bestsellers_date\":{\"$date\":{\"$numberLong\":\"1211587200000\"}},\"description\":\"Odd Thomas, who can communicate with the dead, confronts evil forces in a California coastal town.\",\"price\":{\"$numberInt\":\"27\"},\"published_date\":{\"$date\":{\"$numberLong\":\"1212883200000\"}},\"publisher\":\"Bantam\",\"rank\":{\"$numberInt\":\"1\"},\"rank_last_week\":{\"$numberInt\":\"0\"},\"title\":\"ODD HOURS\",\"weeks_on_list\":{\"$numberInt\":\"1\"},\"new_column\":\"This is a new column\"}'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting dataframe into a RDD of string \n",
    "dataframe.toJSON().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:59.461958Z",
     "start_time": "2020-07-08T19:57:58.691022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\.conda\\envs\\spark\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:88: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Nested StructType not supported in conversion to Arrow\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>bestsellers_date</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>published_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>rank</th>\n",
       "      <th>rank_last_week</th>\n",
       "      <th>title</th>\n",
       "      <th>weeks_on_list</th>\n",
       "      <th>new_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(5b4aa4ead3089013507db18b,)</td>\n",
       "      <td>http://www.amazon.com/Odd-Hours-Dean-Koontz/dp...</td>\n",
       "      <td>Dean R Koontz</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>Odd Thomas, who can communicate with the dead,...</td>\n",
       "      <td>(None, 27)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Bantam</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>ODD HOURS</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>This is a new column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(5b4aa4ead3089013507db18c,)</td>\n",
       "      <td>http://www.amazon.com/The-Host-Novel-Stephenie...</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>Aliens have taken control of the minds and bod...</td>\n",
       "      <td>(25.99, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Little, Brown</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>THE HOST</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>This is a new column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(5b4aa4ead3089013507db18d,)</td>\n",
       "      <td>http://www.amazon.com/Love-Youre-With-Emily-Gi...</td>\n",
       "      <td>Emily Giffin</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>A woman's happy marriage is shaken when she en...</td>\n",
       "      <td>(24.95, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>St. Martin's</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>LOVE THE ONE YOU'RE WITH</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>This is a new column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(5b4aa4ead3089013507db18e,)</td>\n",
       "      <td>http://www.amazon.com/The-Front-Garano-Patrici...</td>\n",
       "      <td>Patricia Cornwell</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>A Massachusetts state investigator and his tea...</td>\n",
       "      <td>(22.95, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Putnam</td>\n",
       "      <td>(4,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>THE FRONT</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>This is a new column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(5b4aa4ead3089013507db18f,)</td>\n",
       "      <td>http://www.amazon.com/Snuff-Chuck-Palahniuk/dp...</td>\n",
       "      <td>Chuck Palahniuk</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>An aging porn queens aims to cap her career by...</td>\n",
       "      <td>(24.95, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>SNUFF</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>This is a new column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10190</th>\n",
       "      <td>(5b4aa4ead3089013507dd959,)</td>\n",
       "      <td>https://www.amazon.com/Clancy-Line-Sight-Jack-...</td>\n",
       "      <td>Mike Maden</td>\n",
       "      <td>((1530921600000,),)</td>\n",
       "      <td>Jack Ryan Jr. risks his life to protect a woma...</td>\n",
       "      <td>(None, 0)</td>\n",
       "      <td>((1532217600000,),)</td>\n",
       "      <td>Putnam</td>\n",
       "      <td>(11,)</td>\n",
       "      <td>(6,)</td>\n",
       "      <td>TOM CLANCY LINE OF SIGHT</td>\n",
       "      <td>(4,)</td>\n",
       "      <td>This is a new column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10191</th>\n",
       "      <td>(5b4aa4ead3089013507dd95a,)</td>\n",
       "      <td>https://www.amazon.com/Something-Water-Novel-C...</td>\n",
       "      <td>Catherine Steadman</td>\n",
       "      <td>((1530921600000,),)</td>\n",
       "      <td>A documentary filmmaker and an investment bank...</td>\n",
       "      <td>(None, 0)</td>\n",
       "      <td>((1532217600000,),)</td>\n",
       "      <td>Ballantine</td>\n",
       "      <td>(12,)</td>\n",
       "      <td>(11,)</td>\n",
       "      <td>SOMETHING IN THE WATER</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>This is a new column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10192</th>\n",
       "      <td>(5b4aa4ead3089013507dd95b,)</td>\n",
       "      <td>https://www.amazon.com/Little-Fires-Everywhere...</td>\n",
       "      <td>Celeste Ng</td>\n",
       "      <td>((1530921600000,),)</td>\n",
       "      <td>An artist upends a quiet town outside Cleveland.</td>\n",
       "      <td>(None, 0)</td>\n",
       "      <td>((1532217600000,),)</td>\n",
       "      <td>Penguin Press</td>\n",
       "      <td>(13,)</td>\n",
       "      <td>(12,)</td>\n",
       "      <td>LITTLE FIRES EVERYWHERE</td>\n",
       "      <td>(41,)</td>\n",
       "      <td>This is a new column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>(5b4aa4ead3089013507dd95c,)</td>\n",
       "      <td>https://www.amazon.com/Shelter-Place-Nora-Robe...</td>\n",
       "      <td>Nora Roberts</td>\n",
       "      <td>((1530921600000,),)</td>\n",
       "      <td>Survivors of a mass shooting outside a mall in...</td>\n",
       "      <td>(None, 0)</td>\n",
       "      <td>((1532217600000,),)</td>\n",
       "      <td>St. Martin's</td>\n",
       "      <td>(14,)</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>SHELTER IN PLACE</td>\n",
       "      <td>(6,)</td>\n",
       "      <td>This is a new column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10194</th>\n",
       "      <td>(5b4aa4ead3089013507dd95d,)</td>\n",
       "      <td>https://www.amazon.com/Last-Time-Lied-Novel/dp...</td>\n",
       "      <td>Riley Sager</td>\n",
       "      <td>((1530921600000,),)</td>\n",
       "      <td>A painter is in danger when she returns to the...</td>\n",
       "      <td>(None, 0)</td>\n",
       "      <td>((1532217600000,),)</td>\n",
       "      <td>Dutton</td>\n",
       "      <td>(15,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>THE LAST TIME I LIED</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>This is a new column</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10195 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               _id  \\\n",
       "0      (5b4aa4ead3089013507db18b,)   \n",
       "1      (5b4aa4ead3089013507db18c,)   \n",
       "2      (5b4aa4ead3089013507db18d,)   \n",
       "3      (5b4aa4ead3089013507db18e,)   \n",
       "4      (5b4aa4ead3089013507db18f,)   \n",
       "...                            ...   \n",
       "10190  (5b4aa4ead3089013507dd959,)   \n",
       "10191  (5b4aa4ead3089013507dd95a,)   \n",
       "10192  (5b4aa4ead3089013507dd95b,)   \n",
       "10193  (5b4aa4ead3089013507dd95c,)   \n",
       "10194  (5b4aa4ead3089013507dd95d,)   \n",
       "\n",
       "                                                     url              author  \\\n",
       "0      http://www.amazon.com/Odd-Hours-Dean-Koontz/dp...       Dean R Koontz   \n",
       "1      http://www.amazon.com/The-Host-Novel-Stephenie...     Stephenie Meyer   \n",
       "2      http://www.amazon.com/Love-Youre-With-Emily-Gi...        Emily Giffin   \n",
       "3      http://www.amazon.com/The-Front-Garano-Patrici...   Patricia Cornwell   \n",
       "4      http://www.amazon.com/Snuff-Chuck-Palahniuk/dp...     Chuck Palahniuk   \n",
       "...                                                  ...                 ...   \n",
       "10190  https://www.amazon.com/Clancy-Line-Sight-Jack-...          Mike Maden   \n",
       "10191  https://www.amazon.com/Something-Water-Novel-C...  Catherine Steadman   \n",
       "10192  https://www.amazon.com/Little-Fires-Everywhere...          Celeste Ng   \n",
       "10193  https://www.amazon.com/Shelter-Place-Nora-Robe...        Nora Roberts   \n",
       "10194  https://www.amazon.com/Last-Time-Lied-Novel/dp...         Riley Sager   \n",
       "\n",
       "          bestsellers_date                                        description  \\\n",
       "0      ((1211587200000,),)  Odd Thomas, who can communicate with the dead,...   \n",
       "1      ((1211587200000,),)  Aliens have taken control of the minds and bod...   \n",
       "2      ((1211587200000,),)  A woman's happy marriage is shaken when she en...   \n",
       "3      ((1211587200000,),)  A Massachusetts state investigator and his tea...   \n",
       "4      ((1211587200000,),)  An aging porn queens aims to cap her career by...   \n",
       "...                    ...                                                ...   \n",
       "10190  ((1530921600000,),)  Jack Ryan Jr. risks his life to protect a woma...   \n",
       "10191  ((1530921600000,),)  A documentary filmmaker and an investment bank...   \n",
       "10192  ((1530921600000,),)   An artist upends a quiet town outside Cleveland.   \n",
       "10193  ((1530921600000,),)  Survivors of a mass shooting outside a mall in...   \n",
       "10194  ((1530921600000,),)  A painter is in danger when she returns to the...   \n",
       "\n",
       "               price       published_date      publisher   rank  \\\n",
       "0         (None, 27)  ((1212883200000,),)         Bantam   (1,)   \n",
       "1      (25.99, None)  ((1212883200000,),)  Little, Brown   (2,)   \n",
       "2      (24.95, None)  ((1212883200000,),)   St. Martin's   (3,)   \n",
       "3      (22.95, None)  ((1212883200000,),)         Putnam   (4,)   \n",
       "4      (24.95, None)  ((1212883200000,),)      Doubleday   (5,)   \n",
       "...              ...                  ...            ...    ...   \n",
       "10190      (None, 0)  ((1532217600000,),)         Putnam  (11,)   \n",
       "10191      (None, 0)  ((1532217600000,),)     Ballantine  (12,)   \n",
       "10192      (None, 0)  ((1532217600000,),)  Penguin Press  (13,)   \n",
       "10193      (None, 0)  ((1532217600000,),)   St. Martin's  (14,)   \n",
       "10194      (None, 0)  ((1532217600000,),)         Dutton  (15,)   \n",
       "\n",
       "      rank_last_week                     title weeks_on_list  \\\n",
       "0               (0,)                 ODD HOURS          (1,)   \n",
       "1               (1,)                  THE HOST          (3,)   \n",
       "2               (2,)  LOVE THE ONE YOU'RE WITH          (2,)   \n",
       "3               (0,)                 THE FRONT          (1,)   \n",
       "4               (0,)                     SNUFF          (1,)   \n",
       "...              ...                       ...           ...   \n",
       "10190           (6,)  TOM CLANCY LINE OF SIGHT          (4,)   \n",
       "10191          (11,)    SOMETHING IN THE WATER          (5,)   \n",
       "10192          (12,)   LITTLE FIRES EVERYWHERE         (41,)   \n",
       "10193           (5,)          SHELTER IN PLACE          (6,)   \n",
       "10194           (0,)      THE LAST TIME I LIED          (1,)   \n",
       "\n",
       "                 new_column  \n",
       "0      This is a new column  \n",
       "1      This is a new column  \n",
       "2      This is a new column  \n",
       "3      This is a new column  \n",
       "4      This is a new column  \n",
       "...                     ...  \n",
       "10190  This is a new column  \n",
       "10191  This is a new column  \n",
       "10192  This is a new column  \n",
       "10193  This is a new column  \n",
       "10194  This is a new column  \n",
       "\n",
       "[10195 rows x 13 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining contents of df as Pandas \n",
    "dataframe.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:57:59.468939Z",
     "start_time": "2020-07-08T19:57:59.463955Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change dicectory for output\n",
    "os.chdir(\"03-processed-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:02.048327Z",
     "start_time": "2020-07-08T19:57:59.471933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write & Save File in .parquet format\n",
    "dataframe.select(\"author\", \"title\", \"rank\", \"description\") \\\n",
    "    .write \\\n",
    "    .save(\"Rankings_Descriptions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:02.441257Z",
     "start_time": "2020-07-08T19:58:02.050296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write & Save File in .json format\n",
    "dataframe.select(\"author\", \"title\") \\\n",
    "    .write \\\n",
    "    .save(\"Authors_Titles.json\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:02.684171Z",
     "start_time": "2020-07-08T19:58:02.443247Z"
    }
   },
   "outputs": [],
   "source": [
    "# End Spark Session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "Material from [Towards Data Science](https://towardsdatascience.com/apache-spark-mllib-tutorial-ec6f1cb336a9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:02.754006Z",
     "start_time": "2020-07-08T19:58:02.686057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:02.973014Z",
     "start_time": "2020-07-08T19:58:02.756855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark sessions\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:03.463675Z",
     "start_time": "2020-07-08T19:58:02.978973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
      "|   crim|  zn|indus|chas|  nox|   rm|  age|   dis|rad|tax|ptratio|     b|lstat|medv|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575| 65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421| 78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998| 45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147| 54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|\n",
      "|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21|28.7|\n",
      "|0.08829|12.5| 7.87|   0|0.524|6.012| 66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|\n",
      "|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2| 396.9|19.15|27.1|\n",
      "|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93|16.5|\n",
      "|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1|18.9|\n",
      "|0.22489|12.5| 7.87|   0|0.524|6.377| 94.3|6.3467|  5|311|   15.2|392.52|20.45|15.0|\n",
      "|0.11747|12.5| 7.87|   0|0.524|6.009| 82.9|6.2267|  5|311|   15.2| 396.9|13.27|18.9|\n",
      "|0.09378|12.5| 7.87|   0|0.524|5.889| 39.0|5.4509|  5|311|   15.2| 390.5|15.71|21.7|\n",
      "|0.62976| 0.0| 8.14|   0|0.538|5.949| 61.8|4.7075|  4|307|   21.0| 396.9| 8.26|20.4|\n",
      "|0.63796| 0.0| 8.14|   0|0.538|6.096| 84.5|4.4619|  4|307|   21.0|380.02|10.26|18.2|\n",
      "|0.62739| 0.0| 8.14|   0|0.538|5.834| 56.5|4.4986|  4|307|   21.0|395.62| 8.47|19.9|\n",
      "|1.05393| 0.0| 8.14|   0|0.538|5.935| 29.3|4.4986|  4|307|   21.0|386.85| 6.58|23.1|\n",
      "| 0.7842| 0.0| 8.14|   0|0.538| 5.99| 81.7|4.2579|  4|307|   21.0|386.75|14.67|17.5|\n",
      "|0.80271| 0.0| 8.14|   0|0.538|5.456| 36.6|3.7965|  4|307|   21.0|288.99|11.69|20.2|\n",
      "| 0.7258| 0.0| 8.14|   0|0.538|5.727| 69.5|3.7965|  4|307|   21.0|390.95|11.28|18.2|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV\n",
    "data = spark.read.csv(\n",
    "    os.path.join(\"02-raw-data\", \"boston_housing.csv\"),\n",
    "    header=True,\n",
    "    inferSchema=True)\n",
    "\n",
    "# Show data\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:03.472651Z",
     "start_time": "2020-07-08T19:58:03.466667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define features (used to predict)\n",
    "feature_columns = data.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:03.521520Z",
     "start_time": "2020-07-08T19:58:03.475646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create features arrays\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:03.821718Z",
     "start_time": "2020-07-08T19:58:03.523515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+--------------------+\n",
      "|   crim|  zn|indus|chas|  nox|   rm|  age|   dis|rad|tax|ptratio|     b|lstat|medv|            features|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+--------------------+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575| 65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|[0.00632,18.0,2.3...|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421| 78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|[0.02731,0.0,7.07...|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|[0.02729,0.0,7.07...|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998| 45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|[0.03237,0.0,2.18...|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147| 54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|[0.06905,0.0,2.18...|\n",
      "|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21|28.7|[0.02985,0.0,2.18...|\n",
      "|0.08829|12.5| 7.87|   0|0.524|6.012| 66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|[0.08829,12.5,7.8...|\n",
      "|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2| 396.9|19.15|27.1|[0.14455,12.5,7.8...|\n",
      "|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93|16.5|[0.21124,12.5,7.8...|\n",
      "|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1|18.9|[0.17004,12.5,7.8...|\n",
      "|0.22489|12.5| 7.87|   0|0.524|6.377| 94.3|6.3467|  5|311|   15.2|392.52|20.45|15.0|[0.22489,12.5,7.8...|\n",
      "|0.11747|12.5| 7.87|   0|0.524|6.009| 82.9|6.2267|  5|311|   15.2| 396.9|13.27|18.9|[0.11747,12.5,7.8...|\n",
      "|0.09378|12.5| 7.87|   0|0.524|5.889| 39.0|5.4509|  5|311|   15.2| 390.5|15.71|21.7|[0.09378,12.5,7.8...|\n",
      "|0.62976| 0.0| 8.14|   0|0.538|5.949| 61.8|4.7075|  4|307|   21.0| 396.9| 8.26|20.4|[0.62976,0.0,8.14...|\n",
      "|0.63796| 0.0| 8.14|   0|0.538|6.096| 84.5|4.4619|  4|307|   21.0|380.02|10.26|18.2|[0.63796,0.0,8.14...|\n",
      "|0.62739| 0.0| 8.14|   0|0.538|5.834| 56.5|4.4986|  4|307|   21.0|395.62| 8.47|19.9|[0.62739,0.0,8.14...|\n",
      "|1.05393| 0.0| 8.14|   0|0.538|5.935| 29.3|4.4986|  4|307|   21.0|386.85| 6.58|23.1|[1.05393,0.0,8.14...|\n",
      "| 0.7842| 0.0| 8.14|   0|0.538| 5.99| 81.7|4.2579|  4|307|   21.0|386.75|14.67|17.5|[0.7842,0.0,8.14,...|\n",
      "|0.80271| 0.0| 8.14|   0|0.538|5.456| 36.6|3.7965|  4|307|   21.0|288.99|11.69|20.2|[0.80271,0.0,8.14...|\n",
      "| 0.7258| 0.0| 8.14|   0|0.538|5.727| 69.5|3.7965|  4|307|   21.0|390.95|11.28|18.2|[0.7258,0.0,8.14,...|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create features columns\n",
    "data_2 = assembler.transform(data)\n",
    "data_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:03.853633Z",
     "start_time": "2020-07-08T19:58:03.824710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing samples\n",
    "train, test = data_2.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:04.194722Z",
     "start_time": "2020-07-08T19:58:03.855627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data count\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:04.356290Z",
     "start_time": "2020-07-08T19:58:04.196716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing data count\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:04.427100Z",
     "start_time": "2020-07-08T19:58:04.359281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define ML algorithm\n",
    "algo = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"medv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:05.232975Z",
     "start_time": "2020-07-08T19:58:04.429094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the Linear Regression model\n",
    "model = algo.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:05.240930Z",
     "start_time": "2020-07-08T19:58:05.233944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.LinearRegressionModel"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model type\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:05.251897Z",
     "start_time": "2020-07-08T19:58:05.243916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_5a659fe74ef4, numFeatures=13"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:05.556082Z",
     "start_time": "2020-07-08T19:58:05.253891Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "evaluation_summary = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:05.564061Z",
     "start_time": "2020-07-08T19:58:05.558078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.198216982300582\n",
      "5.13518205657268\n",
      "0.7113180904343133\n"
     ]
    }
   ],
   "source": [
    "# Check evaluation metrics\n",
    "print(evaluation_summary.meanAbsoluteError)\n",
    "print(evaluation_summary.rootMeanSquaredError)\n",
    "print(evaluation_summary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:05.591990Z",
     "start_time": "2020-07-08T19:58:05.566055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict values for test data\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:05.749565Z",
     "start_time": "2020-07-08T19:58:05.592983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+------+-----+----+-------+---+---+-------+------+-----+----+--------------------+------------------+\n",
      "|   crim|  zn|indus|chas|   nox|   rm| age|    dis|rad|tax|ptratio|     b|lstat|medv|            features|        prediction|\n",
      "+-------+----+-----+----+------+-----+----+-------+---+---+-------+------+-----+----+--------------------+------------------+\n",
      "|0.01439|60.0| 2.93|   0| 0.401|6.604|18.8| 6.2196|  1|265|   15.6| 376.7| 4.38|29.1|[0.01439,60.0,2.9...|31.628181777527303|\n",
      "|0.01778|95.0| 1.47|   0| 0.403|7.135|13.9| 7.6534|  3|402|   17.0| 384.3| 4.45|32.9|[0.01778,95.0,1.4...| 30.92063961116464|\n",
      "|0.02009|95.0| 2.68|   0|0.4161|8.034|31.9|  5.118|  4|224|   14.7|390.55| 2.88|50.0|[0.02009,95.0,2.6...|43.512009757821474|\n",
      "|0.02543|55.0| 3.78|   0| 0.484|6.696|56.4| 5.7321|  5|370|   17.6| 396.9| 7.18|23.9|[0.02543,55.0,3.7...| 27.52185599746684|\n",
      "|0.02763|75.0| 2.95|   0| 0.428|6.595|21.8| 5.4011|  3|252|   18.3|395.63| 4.32|30.8|[0.02763,75.0,2.9...|31.109022066544988|\n",
      "|0.02875|28.0|15.04|   0| 0.464|6.211|28.9| 3.6659|  4|270|   18.2|396.33| 6.21|25.0|[0.02875,28.0,15....| 29.05631321228384|\n",
      "|0.02899|40.0| 1.25|   0| 0.429|6.939|34.5| 8.7921|  1|335|   19.7|389.85| 5.89|26.6|[0.02899,40.0,1.2...|22.469556035838405|\n",
      "|0.03237| 0.0| 2.18|   0| 0.458|6.998|45.8| 6.0622|  3|222|   18.7|394.63| 2.94|33.4|[0.03237,0.0,2.18...| 28.46487023588844|\n",
      "|0.03445|82.5| 2.03|   0| 0.415|6.162|38.4|   6.27|  2|348|   14.7|393.77| 7.43|24.1|[0.03445,82.5,2.0...| 29.10041637787782|\n",
      "|0.03466|35.0| 6.06|   0|0.4379|6.031|23.3| 6.6407|  1|304|   16.9|362.25| 7.83|19.4|[0.03466,35.0,6.0...|23.512649664060277|\n",
      "|0.03705|20.0| 3.33|   0|0.4429|6.968|37.2| 5.2447|  5|216|   14.9|392.23| 4.59|35.4|[0.03705,20.0,3.3...|34.288687285335975|\n",
      "|0.03768|80.0| 1.52|   0| 0.404|7.274|38.3|  7.309|  2|329|   12.6| 392.2| 6.62|34.6|[0.03768,80.0,1.5...|  35.1578983176606|\n",
      "|0.03961| 0.0| 5.19|   0| 0.515|6.037|34.5| 5.9853|  5|224|   20.2| 396.9| 8.01|21.1|[0.03961,0.0,5.19...| 20.35751775166594|\n",
      "|0.04297|52.5| 5.32|   0| 0.405|6.565|22.9| 7.3172|  6|293|   16.6|371.72| 9.51|24.8|[0.04297,52.5,5.3...|27.306144300895312|\n",
      "|0.04301|80.0| 1.91|   0| 0.413|5.663|21.9|10.5857|  4|334|   22.0| 382.8| 8.05|18.2|[0.04301,80.0,1.9...|14.329032960176498|\n",
      "|0.04462|25.0| 4.86|   0| 0.426|6.619|70.4| 5.4007|  4|281|   19.0|395.63| 7.22|23.9|[0.04462,25.0,4.8...| 26.77590071317647|\n",
      "|0.04527| 0.0|11.93|   0| 0.573| 6.12|76.7| 2.2875|  1|273|   21.0| 396.9| 9.08|20.6|[0.04527,0.0,11.9...|21.952062176439433|\n",
      "| 0.0459|52.5| 5.32|   0| 0.405|6.315|45.6| 7.3172|  6|293|   16.6| 396.9|  7.6|22.3|[0.0459,52.5,5.32...|27.219063012425163|\n",
      "|0.05059| 0.0| 4.49|   0| 0.449|6.389|48.0| 4.7794|  3|247|   18.5| 396.9| 9.62|23.9|[0.05059,0.0,4.49...| 24.64140565062831|\n",
      "| 0.0536|21.0| 5.64|   0| 0.439|6.511|21.1| 6.8147|  4|243|   16.8| 396.9| 5.28|25.0|[0.0536,21.0,5.64...|27.762942879105033|\n",
      "+-------+----+-----+----+------+-----+----+-------+---+---+-------+------+-----+----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show predictions\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:05.927092Z",
     "start_time": "2020-07-08T19:58:05.752558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "|medv|            features|        prediction|\n",
      "+----+--------------------+------------------+\n",
      "|29.1|[0.01439,60.0,2.9...|31.628181777527303|\n",
      "|32.9|[0.01778,95.0,1.4...| 30.92063961116464|\n",
      "|50.0|[0.02009,95.0,2.6...|43.512009757821474|\n",
      "|23.9|[0.02543,55.0,3.7...| 27.52185599746684|\n",
      "|30.8|[0.02763,75.0,2.9...|31.109022066544988|\n",
      "|25.0|[0.02875,28.0,15....| 29.05631321228384|\n",
      "|26.6|[0.02899,40.0,1.2...|22.469556035838405|\n",
      "|33.4|[0.03237,0.0,2.18...| 28.46487023588844|\n",
      "|24.1|[0.03445,82.5,2.0...| 29.10041637787782|\n",
      "|19.4|[0.03466,35.0,6.0...|23.512649664060277|\n",
      "|35.4|[0.03705,20.0,3.3...|34.288687285335975|\n",
      "|34.6|[0.03768,80.0,1.5...|  35.1578983176606|\n",
      "|21.1|[0.03961,0.0,5.19...| 20.35751775166594|\n",
      "|24.8|[0.04297,52.5,5.3...|27.306144300895312|\n",
      "|18.2|[0.04301,80.0,1.9...|14.329032960176498|\n",
      "|23.9|[0.04462,25.0,4.8...| 26.77590071317647|\n",
      "|20.6|[0.04527,0.0,11.9...|21.952062176439433|\n",
      "|22.3|[0.0459,52.5,5.32...|27.219063012425163|\n",
      "|23.9|[0.05059,0.0,4.49...| 24.64140565062831|\n",
      "|25.0|[0.0536,21.0,5.64...|27.762942879105033|\n",
      "+----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show predictions\n",
    "predictions.select(predictions.columns[13:]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation\n",
    "\n",
    "Material from [Towards Data Science](https://towardsdatascience.com/apache-spark-mllib-tutorial-7aba8a1dce6e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:05.934072Z",
     "start_time": "2020-07-08T19:58:05.929085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:06.167450Z",
     "start_time": "2020-07-08T19:58:05.945042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| color|\n",
      "+---+------+\n",
      "|  1|   red|\n",
      "|  2|  blue|\n",
      "|  3|orange|\n",
      "|  4| white|\n",
      "|  5|   red|\n",
      "|  6|orange|\n",
      "|  7|   red|\n",
      "|  8| white|\n",
      "|  9|   red|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Spaek sessuin\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read data from CSV\n",
    "data = spark.read.csv(\n",
    "    os.path.join(\"02-raw-data\", \"colors.csv\"),\n",
    "    header=True,\n",
    "    inferSchema=True)\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:06.191386Z",
     "start_time": "2020-07-08T19:58:06.170439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.feature.StringIndexer"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create indexer\n",
    "indexer = StringIndexer(inputCol=\"color\", outputCol=\"color_indexed\")\n",
    "type(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:06.764879Z",
     "start_time": "2020-07-08T19:58:06.194392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n",
      "| id| color|color_indexed|\n",
      "+---+------+-------------+\n",
      "|  1|   red|          0.0|\n",
      "|  2|  blue|          3.0|\n",
      "|  3|orange|          1.0|\n",
      "|  4| white|          2.0|\n",
      "|  5|   red|          0.0|\n",
      "|  6|orange|          1.0|\n",
      "|  7|   red|          0.0|\n",
      "|  8| white|          2.0|\n",
      "|  9|   red|          0.0|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Learn mappings from color label to color index\n",
    "indexer_model = indexer.fit(data)\n",
    "\n",
    "# Created indexed data\n",
    "indexed_data = indexer_model.transform(data)\n",
    "\n",
    "indexed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:06.783822Z",
     "start_time": "2020-07-08T19:58:06.766869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create OneHotEncoder\n",
    "ohe = OneHotEncoder(\n",
    "    inputCols=[\"color_indexed\"],\n",
    "    outputCols=[\"color_ohe\"]) #,\n",
    "    #dropLast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:06.821722Z",
     "start_time": "2020-07-08T19:58:06.785817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoderModel: uid=OneHotEncoder_31816ce18f35, dropLast=true, handleInvalid=error, numInputCols=1, numOutputCols=1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the estimator\n",
    "ohe_model = ohe.fit(indexed_data)\n",
    "ohe_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:06.938411Z",
     "start_time": "2020-07-08T19:58:06.823716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+-------------+\n",
      "| id| color|color_indexed|    color_ohe|\n",
      "+---+------+-------------+-------------+\n",
      "|  1|   red|          0.0|(3,[0],[1.0])|\n",
      "|  2|  blue|          3.0|    (3,[],[])|\n",
      "|  3|orange|          1.0|(3,[1],[1.0])|\n",
      "|  4| white|          2.0|(3,[2],[1.0])|\n",
      "|  5|   red|          0.0|(3,[0],[1.0])|\n",
      "|  6|orange|          1.0|(3,[1],[1.0])|\n",
      "|  7|   red|          0.0|(3,[0],[1.0])|\n",
      "|  8| white|          2.0|(3,[2],[1.0])|\n",
      "|  9|   red|          0.0|(3,[0],[1.0])|\n",
      "+---+------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply trained model to data\n",
    "encoded_data = ohe_model.transform(indexed_data)\n",
    "encoded_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:07.186747Z",
     "start_time": "2020-07-08T19:58:06.940407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+\n",
      "|_c0|  _c1| _c2| _c3| _c4|_c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|\n",
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+\n",
      "|  1|14.23|1.71|2.43|15.6|127| 2.8|3.06|0.28|2.29|5.64|1.04|3.92|1065|\n",
      "|  1| 13.2|1.78|2.14|11.2|100|2.65|2.76|0.26|1.28|4.38|1.05| 3.4|1050|\n",
      "|  1|13.16|2.36|2.67|18.6|101| 2.8|3.24| 0.3|2.81|5.68|1.03|3.17|1185|\n",
      "|  1|14.37|1.95| 2.5|16.8|113|3.85|3.49|0.24|2.18| 7.8|0.86|3.45|1480|\n",
      "|  1|13.24|2.59|2.87|21.0|118| 2.8|2.69|0.39|1.82|4.32|1.04|2.93| 735|\n",
      "|  1| 14.2|1.76|2.45|15.2|112|3.27|3.39|0.34|1.97|6.75|1.05|2.85|1450|\n",
      "|  1|14.39|1.87|2.45|14.6| 96| 2.5|2.52| 0.3|1.98|5.25|1.02|3.58|1290|\n",
      "|  1|14.06|2.15|2.61|17.6|121| 2.6|2.51|0.31|1.25|5.05|1.06|3.58|1295|\n",
      "|  1|14.83|1.64|2.17|14.0| 97| 2.8|2.98|0.29|1.98| 5.2|1.08|2.85|1045|\n",
      "|  1|13.86|1.35|2.27|16.0| 98|2.98|3.15|0.22|1.85|7.22|1.01|3.55|1045|\n",
      "|  1| 14.1|2.16| 2.3|18.0|105|2.95|3.32|0.22|2.38|5.75|1.25|3.17|1510|\n",
      "|  1|14.12|1.48|2.32|16.8| 95| 2.2|2.43|0.26|1.57| 5.0|1.17|2.82|1280|\n",
      "|  1|13.75|1.73|2.41|16.0| 89| 2.6|2.76|0.29|1.81| 5.6|1.15| 2.9|1320|\n",
      "|  1|14.75|1.73|2.39|11.4| 91| 3.1|3.69|0.43|2.81| 5.4|1.25|2.73|1150|\n",
      "|  1|14.38|1.87|2.38|12.0|102| 3.3|3.64|0.29|2.96| 7.5| 1.2| 3.0|1547|\n",
      "|  1|13.63|1.81| 2.7|17.2|112|2.85|2.91| 0.3|1.46| 7.3|1.28|2.88|1310|\n",
      "|  1| 14.3|1.92|2.72|20.0|120| 2.8|3.14|0.33|1.97| 6.2|1.07|2.65|1280|\n",
      "|  1|13.83|1.57|2.62|20.0|115|2.95| 3.4| 0.4|1.72| 6.6|1.13|2.57|1130|\n",
      "|  1|14.19|1.59|2.48|16.5|108| 3.3|3.93|0.32|1.86| 8.7|1.23|2.82|1680|\n",
      "|  1|13.64| 3.1|2.56|15.2|116| 2.7|3.03|0.17|1.66| 5.1|0.96|3.36| 845|\n",
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV\n",
    "data = spark.read.csv(\n",
    "    os.path.join(\"02-raw-data\", \"wine.csv\"),\n",
    "    header=False, inferSchema=True)\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:07.305430Z",
     "start_time": "2020-07-08T19:58:07.189740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+--------------------+\n",
      "|_c0|  _c1| _c2| _c3| _c4|_c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|            features|\n",
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+--------------------+\n",
      "|  1|14.23|1.71|2.43|15.6|127| 2.8|3.06|0.28|2.29|5.64|1.04|3.92|1065|[14.23,1.71,2.43,...|\n",
      "|  1| 13.2|1.78|2.14|11.2|100|2.65|2.76|0.26|1.28|4.38|1.05| 3.4|1050|[13.2,1.78,2.14,1...|\n",
      "|  1|13.16|2.36|2.67|18.6|101| 2.8|3.24| 0.3|2.81|5.68|1.03|3.17|1185|[13.16,2.36,2.67,...|\n",
      "|  1|14.37|1.95| 2.5|16.8|113|3.85|3.49|0.24|2.18| 7.8|0.86|3.45|1480|[14.37,1.95,2.5,1...|\n",
      "|  1|13.24|2.59|2.87|21.0|118| 2.8|2.69|0.39|1.82|4.32|1.04|2.93| 735|[13.24,2.59,2.87,...|\n",
      "|  1| 14.2|1.76|2.45|15.2|112|3.27|3.39|0.34|1.97|6.75|1.05|2.85|1450|[14.2,1.76,2.45,1...|\n",
      "|  1|14.39|1.87|2.45|14.6| 96| 2.5|2.52| 0.3|1.98|5.25|1.02|3.58|1290|[14.39,1.87,2.45,...|\n",
      "|  1|14.06|2.15|2.61|17.6|121| 2.6|2.51|0.31|1.25|5.05|1.06|3.58|1295|[14.06,2.15,2.61,...|\n",
      "|  1|14.83|1.64|2.17|14.0| 97| 2.8|2.98|0.29|1.98| 5.2|1.08|2.85|1045|[14.83,1.64,2.17,...|\n",
      "|  1|13.86|1.35|2.27|16.0| 98|2.98|3.15|0.22|1.85|7.22|1.01|3.55|1045|[13.86,1.35,2.27,...|\n",
      "|  1| 14.1|2.16| 2.3|18.0|105|2.95|3.32|0.22|2.38|5.75|1.25|3.17|1510|[14.1,2.16,2.3,18...|\n",
      "|  1|14.12|1.48|2.32|16.8| 95| 2.2|2.43|0.26|1.57| 5.0|1.17|2.82|1280|[14.12,1.48,2.32,...|\n",
      "|  1|13.75|1.73|2.41|16.0| 89| 2.6|2.76|0.29|1.81| 5.6|1.15| 2.9|1320|[13.75,1.73,2.41,...|\n",
      "|  1|14.75|1.73|2.39|11.4| 91| 3.1|3.69|0.43|2.81| 5.4|1.25|2.73|1150|[14.75,1.73,2.39,...|\n",
      "|  1|14.38|1.87|2.38|12.0|102| 3.3|3.64|0.29|2.96| 7.5| 1.2| 3.0|1547|[14.38,1.87,2.38,...|\n",
      "|  1|13.63|1.81| 2.7|17.2|112|2.85|2.91| 0.3|1.46| 7.3|1.28|2.88|1310|[13.63,1.81,2.7,1...|\n",
      "|  1| 14.3|1.92|2.72|20.0|120| 2.8|3.14|0.33|1.97| 6.2|1.07|2.65|1280|[14.3,1.92,2.72,2...|\n",
      "|  1|13.83|1.57|2.62|20.0|115|2.95| 3.4| 0.4|1.72| 6.6|1.13|2.57|1130|[13.83,1.57,2.62,...|\n",
      "|  1|14.19|1.59|2.48|16.5|108| 3.3|3.93|0.32|1.86| 8.7|1.23|2.82|1680|[14.19,1.59,2.48,...|\n",
      "|  1|13.64| 3.1|2.56|15.2|116| 2.7|3.03|0.17|1.66| 5.1|0.96|3.36| 845|[13.64,3.1,2.56,1...|\n",
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=data.columns[1:],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "data_2 = assembler.transform(data)\n",
    "data_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:07.324377Z",
     "start_time": "2020-07-08T19:58:07.308422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create scaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:07.503897Z",
     "start_time": "2020-07-08T19:58:07.326373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scaler model\n",
    "scaler_model = scaler.fit(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:07.689402Z",
     "start_time": "2020-07-08T19:58:07.506891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+--------------------+--------------------+\n",
      "|_c0|  _c1| _c2| _c3| _c4|_c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|            features|     scaled_features|\n",
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+--------------------+--------------------+\n",
      "|  1|14.23|1.71|2.43|15.6|127| 2.8|3.06|0.28|2.29|5.64|1.04|3.92|1065|[14.23,1.71,2.43,...|[17.5283750084766...|\n",
      "|  1| 13.2|1.78|2.14|11.2|100|2.65|2.76|0.26|1.28|4.38|1.05| 3.4|1050|[13.2,1.78,2.14,1...|[16.2596310690015...|\n",
      "|  1|13.16|2.36|2.67|18.6|101| 2.8|3.24| 0.3|2.81|5.68|1.03|3.17|1185|[13.16,2.36,2.67,...|[16.2103594597015...|\n",
      "|  1|14.37|1.95| 2.5|16.8|113|3.85|3.49|0.24|2.18| 7.8|0.86|3.45|1480|[14.37,1.95,2.5,1...|[17.7008256410266...|\n",
      "|  1|13.24|2.59|2.87|21.0|118| 2.8|2.69|0.39|1.82|4.32|1.04|2.93| 735|[13.24,2.59,2.87,...|[16.3089026783015...|\n",
      "|  1| 14.2|1.76|2.45|15.2|112|3.27|3.39|0.34|1.97|6.75|1.05|2.85|1450|[14.2,1.76,2.45,1...|[17.4914213015016...|\n",
      "|  1|14.39|1.87|2.45|14.6| 96| 2.5|2.52| 0.3|1.98|5.25|1.02|3.58|1290|[14.39,1.87,2.45,...|[17.7254614456766...|\n",
      "|  1|14.06|2.15|2.61|17.6|121| 2.6|2.51|0.31|1.25|5.05|1.06|3.58|1295|[14.06,2.15,2.61,...|[17.3189706689516...|\n",
      "|  1|14.83|1.64|2.17|14.0| 97| 2.8|2.98|0.29|1.98| 5.2|1.08|2.85|1045|[14.83,1.64,2.17,...|[18.2674491479767...|\n",
      "|  1|13.86|1.35|2.27|16.0| 98|2.98|3.15|0.22|1.85|7.22|1.01|3.55|1045|[13.86,1.35,2.27,...|[17.0726126224516...|\n",
      "|  1| 14.1|2.16| 2.3|18.0|105|2.95|3.32|0.22|2.38|5.75|1.25|3.17|1510|[14.1,2.16,2.3,18...|[17.3682422782516...|\n",
      "|  1|14.12|1.48|2.32|16.8| 95| 2.2|2.43|0.26|1.57| 5.0|1.17|2.82|1280|[14.12,1.48,2.32,...|[17.3928780829016...|\n",
      "|  1|13.75|1.73|2.41|16.0| 89| 2.6|2.76|0.29|1.81| 5.6|1.15| 2.9|1320|[13.75,1.73,2.41,...|[16.9371156968765...|\n",
      "|  1|14.75|1.73|2.39|11.4| 91| 3.1|3.69|0.43|2.81| 5.4|1.25|2.73|1150|[14.75,1.73,2.39,...|[18.1689059293767...|\n",
      "|  1|14.38|1.87|2.38|12.0|102| 3.3|3.64|0.29|2.96| 7.5| 1.2| 3.0|1547|[14.38,1.87,2.38,...|[17.7131435433516...|\n",
      "|  1|13.63|1.81| 2.7|17.2|112|2.85|2.91| 0.3|1.46| 7.3|1.28|2.88|1310|[13.63,1.81,2.7,1...|[16.7893008689765...|\n",
      "|  1| 14.3|1.92|2.72|20.0|120| 2.8|3.14|0.33|1.97| 6.2|1.07|2.65|1280|[14.3,1.92,2.72,2...|[17.6146003247516...|\n",
      "|  1|13.83|1.57|2.62|20.0|115|2.95| 3.4| 0.4|1.72| 6.6|1.13|2.57|1130|[13.83,1.57,2.62,...|[17.0356589154766...|\n",
      "|  1|14.19|1.59|2.48|16.5|108| 3.3|3.93|0.32|1.86| 8.7|1.23|2.82|1680|[14.19,1.59,2.48,...|[17.4791033991766...|\n",
      "|  1|13.64| 3.1|2.56|15.2|116| 2.7|3.03|0.17|1.66| 5.1|0.96|3.36| 845|[13.64,3.1,2.56,1...|[16.8016187713015...|\n",
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scaled data\n",
    "scaled_data = scaler_model.transform(data_2)\n",
    "scaled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:07.970650Z",
     "start_time": "2020-07-08T19:58:07.692394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+--------------------+--------------------+\n",
      "|_c0|  _c1| _c2| _c3| _c4|_c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|            features|     features_minmax|\n",
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+--------------------+--------------------+\n",
      "|  1|14.23|1.71|2.43|15.6|127| 2.8|3.06|0.28|2.29|5.64|1.04|3.92|1065|[14.23,1.71,2.43,...|[0.84210526315789...|\n",
      "|  1| 13.2|1.78|2.14|11.2|100|2.65|2.76|0.26|1.28|4.38|1.05| 3.4|1050|[13.2,1.78,2.14,1...|[0.57105263157894...|\n",
      "|  1|13.16|2.36|2.67|18.6|101| 2.8|3.24| 0.3|2.81|5.68|1.03|3.17|1185|[13.16,2.36,2.67,...|[0.56052631578947...|\n",
      "|  1|14.37|1.95| 2.5|16.8|113|3.85|3.49|0.24|2.18| 7.8|0.86|3.45|1480|[14.37,1.95,2.5,1...|[0.87894736842105...|\n",
      "|  1|13.24|2.59|2.87|21.0|118| 2.8|2.69|0.39|1.82|4.32|1.04|2.93| 735|[13.24,2.59,2.87,...|[0.58157894736842...|\n",
      "|  1| 14.2|1.76|2.45|15.2|112|3.27|3.39|0.34|1.97|6.75|1.05|2.85|1450|[14.2,1.76,2.45,1...|[0.83421052631578...|\n",
      "|  1|14.39|1.87|2.45|14.6| 96| 2.5|2.52| 0.3|1.98|5.25|1.02|3.58|1290|[14.39,1.87,2.45,...|[0.88421052631578...|\n",
      "|  1|14.06|2.15|2.61|17.6|121| 2.6|2.51|0.31|1.25|5.05|1.06|3.58|1295|[14.06,2.15,2.61,...|[0.79736842105263...|\n",
      "|  1|14.83|1.64|2.17|14.0| 97| 2.8|2.98|0.29|1.98| 5.2|1.08|2.85|1045|[14.83,1.64,2.17,...|[0.99999999999999...|\n",
      "|  1|13.86|1.35|2.27|16.0| 98|2.98|3.15|0.22|1.85|7.22|1.01|3.55|1045|[13.86,1.35,2.27,...|[0.74473684210526...|\n",
      "|  1| 14.1|2.16| 2.3|18.0|105|2.95|3.32|0.22|2.38|5.75|1.25|3.17|1510|[14.1,2.16,2.3,18...|[0.80789473684210...|\n",
      "|  1|14.12|1.48|2.32|16.8| 95| 2.2|2.43|0.26|1.57| 5.0|1.17|2.82|1280|[14.12,1.48,2.32,...|[0.81315789473684...|\n",
      "|  1|13.75|1.73|2.41|16.0| 89| 2.6|2.76|0.29|1.81| 5.6|1.15| 2.9|1320|[13.75,1.73,2.41,...|[0.71578947368421...|\n",
      "|  1|14.75|1.73|2.39|11.4| 91| 3.1|3.69|0.43|2.81| 5.4|1.25|2.73|1150|[14.75,1.73,2.39,...|[0.97894736842105...|\n",
      "|  1|14.38|1.87|2.38|12.0|102| 3.3|3.64|0.29|2.96| 7.5| 1.2| 3.0|1547|[14.38,1.87,2.38,...|[0.88157894736842...|\n",
      "|  1|13.63|1.81| 2.7|17.2|112|2.85|2.91| 0.3|1.46| 7.3|1.28|2.88|1310|[13.63,1.81,2.7,1...|[0.68421052631578...|\n",
      "|  1| 14.3|1.92|2.72|20.0|120| 2.8|3.14|0.33|1.97| 6.2|1.07|2.65|1280|[14.3,1.92,2.72,2...|[0.86052631578947...|\n",
      "|  1|13.83|1.57|2.62|20.0|115|2.95| 3.4| 0.4|1.72| 6.6|1.13|2.57|1130|[13.83,1.57,2.62,...|[0.73684210526315...|\n",
      "|  1|14.19|1.59|2.48|16.5|108| 3.3|3.93|0.32|1.86| 8.7|1.23|2.82|1680|[14.19,1.59,2.48,...|[0.83157894736842...|\n",
      "|  1|13.64| 3.1|2.56|15.2|116| 2.7|3.03|0.17|1.66| 5.1|0.96|3.36| 845|[13.64,3.1,2.56,1...|[0.68684210526315...|\n",
      "+---+-----+----+----+----+---+----+----+----+----+----+----+----+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min/max scaler\n",
    "scaler = MinMaxScaler(min=0, max=1, inputCol='features',\n",
    "                      outputCol='features_minmax')\n",
    "\n",
    "scaler_model = scaler.fit(data_2)\n",
    "data_3 = scaler_model.transform(data_2)\n",
    "data_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:09.569379Z",
     "start_time": "2020-07-08T19:58:07.973644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from CSV\n",
    "data = spark.read.csv(\n",
    "    os.path.join(\"02-raw-data\", \"digits.csv\"),\n",
    "    header=True,\n",
    "    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:10.215649Z",
     "start_time": "2020-07-08T19:58:09.571372Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create assembler\n",
    "assembler = VectorAssembler(inputCols=data.columns[1:], outputCol='features')\n",
    "data_2 = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:21.695961Z",
     "start_time": "2020-07-08T19:58:10.217644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        features_pca|\n",
      "+--------------------+\n",
      "|[103.738813757982...|\n",
      "|[2466.78627830941...|\n",
      "|[-121.55984060478...|\n",
      "|[599.578991071953...|\n",
      "|[2689.04430947598...|\n",
      "|[1253.08650413365...|\n",
      "|[93.0114290617962...|\n",
      "|[650.952778816163...|\n",
      "|[1115.56395904828...|\n",
      "|[1062.72668192116...|\n",
      "|[1029.01690081557...|\n",
      "|[458.805321389768...|\n",
      "|[-200.34133976162...|\n",
      "|[751.263926957183...|\n",
      "|[1265.44211418056...|\n",
      "|[-199.11010313256...|\n",
      "|[762.715694923041...|\n",
      "|[1744.79986516159...|\n",
      "|[128.314928856543...|\n",
      "|[1731.44148649029...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Principal component analysis \n",
    "pca = PCA(k=2, inputCol='features', outputCol='features_pca')\n",
    "pca_model = pca.fit(data_2)\n",
    "pca_data = pca_model.transform(data_2).select('features_pca')\n",
    "pca_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Material from [Towards Data Science](https://towardsdatascience.com/apache-spark-mllib-tutorial-part-3-complete-classification-workflow-a1eb430ad069)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:21.702942Z",
     "start_time": "2020-07-08T19:58:21.697956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:21.953273Z",
     "start_time": "2020-07-08T19:58:21.704939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|Gender| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV\n",
    "data = spark.read.csv(\n",
    "    os.path.join(\"02-raw-data\", \"titanic.csv\"),\n",
    "    header=True, inferSchema=True)\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:22.743162Z",
     "start_time": "2020-07-08T19:58:21.955269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>714</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>446.0</td>\n",
       "      <td>0.3838383838383838</td>\n",
       "      <td>2.308641975308642</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29.69911764705882</td>\n",
       "      <td>0.5230078563411896</td>\n",
       "      <td>0.38159371492704824</td>\n",
       "      <td>260318.54916792738</td>\n",
       "      <td>32.2042079685746</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>257.3538420152301</td>\n",
       "      <td>0.48659245426485753</td>\n",
       "      <td>0.8360712409770491</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14.526497332334035</td>\n",
       "      <td>1.1027434322934315</td>\n",
       "      <td>0.8060572211299488</td>\n",
       "      <td>471609.26868834975</td>\n",
       "      <td>49.69342859718089</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"</td>\n",
       "      <td>female</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A10</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>891</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>WE/P 5735</td>\n",
       "      <td>512.3292</td>\n",
       "      <td>T</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary        PassengerId             Survived              Pclass  \\\n",
       "0   count                891                  891                 891   \n",
       "1    mean              446.0   0.3838383838383838   2.308641975308642   \n",
       "2  stddev  257.3538420152301  0.48659245426485753  0.8360712409770491   \n",
       "3     min                  1                    0                   1   \n",
       "4     max                891                    1                   3   \n",
       "\n",
       "                                               Name  Gender  \\\n",
       "0                                               891     891   \n",
       "1                                              None    None   \n",
       "2                                              None    None   \n",
       "3  \"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"  female   \n",
       "4                       van Melkebeke, Mr. Philemon    male   \n",
       "\n",
       "                  Age               SibSp                Parch  \\\n",
       "0                 714                 891                  891   \n",
       "1   29.69911764705882  0.5230078563411896  0.38159371492704824   \n",
       "2  14.526497332334035  1.1027434322934315   0.8060572211299488   \n",
       "3                0.42                   0                    0   \n",
       "4                80.0                   8                    6   \n",
       "\n",
       "               Ticket               Fare Cabin Embarked  \n",
       "0                 891                891   204      889  \n",
       "1  260318.54916792738   32.2042079685746  None     None  \n",
       "2  471609.26868834975  49.69342859718089  None     None  \n",
       "3              110152                0.0   A10        C  \n",
       "4           WE/P 5735           512.3292     T        S  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary stats in Pandas\n",
    "data.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:22.771090Z",
     "start_time": "2020-07-08T19:58:22.745157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select columns\n",
    "data = data.select(['Survived', 'Pclass', 'Gender',\n",
    "                    'Age', 'SibSp', 'Parch', 'Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:23.164039Z",
     "start_time": "2020-07-08T19:58:22.774081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+-----------------+\n",
      "|Survived|Pclass|Gender| Age|SibSp|Parch|   Fare|       AgeImputed|\n",
      "+--------+------+------+----+-----+-----+-------+-----------------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|             22.0|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|             38.0|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|             26.0|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|             35.0|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|             35.0|\n",
      "|       0|     3|  male|null|    0|    0| 8.4583|29.69911764705882|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|             54.0|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|              2.0|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|             27.0|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|             14.0|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|              4.0|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|             58.0|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|             20.0|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|             39.0|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|             14.0|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|             55.0|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|              2.0|\n",
      "|       1|     2|  male|null|    0|    0|   13.0|29.69911764705882|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|             31.0|\n",
      "|       1|     3|female|null|    0|    0|  7.225|29.69911764705882|\n",
      "+--------+------+------+----+-----+-----+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imputer (missing values)\n",
    "imputer = Imputer(strategy='mean', inputCols=[\n",
    "                  'Age'], outputCols=['AgeImputed'])\n",
    "imputer_model = imputer.fit(data)\n",
    "data = imputer_model.transform(data)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:23.479196Z",
     "start_time": "2020-07-08T19:58:23.167030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+-----------------+-------------+\n",
      "|Survived|Pclass|Gender| Age|SibSp|Parch|   Fare|       AgeImputed|GenderIndexed|\n",
      "+--------+------+------+----+-----+-----+-------+-----------------+-------------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|             22.0|          0.0|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|             38.0|          1.0|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|             26.0|          1.0|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|             35.0|          1.0|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|             35.0|          0.0|\n",
      "|       0|     3|  male|null|    0|    0| 8.4583|29.69911764705882|          0.0|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|             54.0|          0.0|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|              2.0|          0.0|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|             27.0|          1.0|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|             14.0|          1.0|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|              4.0|          1.0|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|             58.0|          1.0|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|             20.0|          0.0|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|             39.0|          0.0|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|             14.0|          1.0|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|             55.0|          1.0|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|              2.0|          0.0|\n",
      "|       1|     2|  male|null|    0|    0|   13.0|29.69911764705882|          0.0|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|             31.0|          1.0|\n",
      "|       1|     3|female|null|    0|    0|  7.225|29.69911764705882|          1.0|\n",
      "+--------+------+------+----+-----+-----+-------+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index data\n",
    "gender_indexer = StringIndexer(\n",
    "    inputCol='Gender', outputCol='GenderIndexed'\n",
    ")\n",
    "gender_indexer_model = gender_indexer.fit(data)\n",
    "data = gender_indexer_model.transform(data)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:23.615831Z",
     "start_time": "2020-07-08T19:58:23.481191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+-----------------+-------------+--------------------+\n",
      "|Survived|Pclass|Gender| Age|SibSp|Parch|   Fare|       AgeImputed|GenderIndexed|            features|\n",
      "+--------+------+------+----+-----+-----+-------+-----------------+-------------+--------------------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|             22.0|          0.0|[3.0,1.0,0.0,7.25...|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|             38.0|          1.0|[1.0,1.0,0.0,71.2...|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|             26.0|          1.0|[3.0,0.0,0.0,7.92...|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|             35.0|          1.0|[1.0,1.0,0.0,53.1...|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|             35.0|          0.0|[3.0,0.0,0.0,8.05...|\n",
      "|       0|     3|  male|null|    0|    0| 8.4583|29.69911764705882|          0.0|[3.0,0.0,0.0,8.45...|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|             54.0|          0.0|[1.0,0.0,0.0,51.8...|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|              2.0|          0.0|[3.0,3.0,1.0,21.0...|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|             27.0|          1.0|[3.0,0.0,2.0,11.1...|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|             14.0|          1.0|[2.0,1.0,0.0,30.0...|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|              4.0|          1.0|[3.0,1.0,1.0,16.7...|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|             58.0|          1.0|[1.0,0.0,0.0,26.5...|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|             20.0|          0.0|[3.0,0.0,0.0,8.05...|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|             39.0|          0.0|[3.0,1.0,5.0,31.2...|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|             14.0|          1.0|[3.0,0.0,0.0,7.85...|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|             55.0|          1.0|[2.0,0.0,0.0,16.0...|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|              2.0|          0.0|[3.0,4.0,1.0,29.1...|\n",
      "|       1|     2|  male|null|    0|    0|   13.0|29.69911764705882|          0.0|[2.0,0.0,0.0,13.0...|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|             31.0|          1.0|[3.0,1.0,0.0,18.0...|\n",
      "|       1|     3|female|null|    0|    0|  7.225|29.69911764705882|          1.0|[3.0,0.0,0.0,7.22...|\n",
      "+--------+------+------+----+-----+-----+-------+-----------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Pclass', 'SibSp', 'Parch', 'Fare', 'AgeImputed', 'GenderIndexed'], outputCol='features')\n",
    "data = assembler.transform(data)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:24.891419Z",
     "start_time": "2020-07-08T19:58:23.617824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Algorithm and model\n",
    "algo = RandomForestClassifier(featuresCol='features', labelCol='Survived')\n",
    "model = algo.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:25.081912Z",
     "start_time": "2020-07-08T19:58:24.893414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|Survived|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       0|       0.0|[0.89551394491524...|\n",
      "|       1|       1.0|[0.03340837227568...|\n",
      "|       1|       1.0|[0.43427464885979...|\n",
      "|       1|       1.0|[0.05957664990639...|\n",
      "|       0|       0.0|[0.87195589265551...|\n",
      "|       0|       0.0|[0.87195589265551...|\n",
      "|       0|       0.0|[0.75135888964194...|\n",
      "|       0|       0.0|[0.71124488011266...|\n",
      "|       1|       1.0|[0.43979222491577...|\n",
      "|       1|       1.0|[0.09010631586934...|\n",
      "|       1|       1.0|[0.39443761678913...|\n",
      "|       1|       1.0|[0.09000718096115...|\n",
      "|       0|       0.0|[0.87825208756381...|\n",
      "|       0|       0.0|[0.86089348748872...|\n",
      "|       0|       1.0|[0.30549999071718...|\n",
      "|       1|       1.0|[0.28043408460639...|\n",
      "|       0|       0.0|[0.75132804256424...|\n",
      "|       1|       0.0|[0.84628105917486...|\n",
      "|       0|       0.0|[0.53498851756044...|\n",
      "|       1|       1.0|[0.32864593954239...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "predictions = model.transform(data)\n",
    "predictions.select(['Survived','prediction', 'probability']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:25.423997Z",
     "start_time": "2020-07-08T19:58:25.083905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.898313254295423"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate predictions\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='Survived', metricName='areaUnderROC')\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:25.646401Z",
     "start_time": "2020-07-08T19:58:25.426988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Isolate data for use with scitkit-learn\n",
    "y_true = predictions.select(['Survived']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:25.655377Z",
     "start_time": "2020-07-08T19:58:25.648396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_true[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:25.690284Z",
     "start_time": "2020-07-08T19:58:25.658370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0),\n",
       " Row(Survived=1),\n",
       " Row(Survived=0)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:25.724225Z",
     "start_time": "2020-07-08T19:58:25.693276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=1.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0),\n",
       " Row(prediction=0.0)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T19:58:25.730186Z",
     "start_time": "2020-07-08T19:58:25.726188Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scikit-learn\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# print(classification_report(y_true, y_pred))\n",
    "# print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources.html\n",
    "\n",
    "https://github.com/apache/spark/tree/master/examples/src/main/python\n",
    "\n",
    "https://spark.apache.org/docs/latest/\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/quick-start.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/mllib-data-types.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "\n",
    "https://medium.com/@jaafarbenabderrazak.info/spark-for-machine-learning-using-python-and-mllib-435efdc3f708\n",
    "\n",
    "https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.html\n",
    "\n",
    "https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes\n",
    "\n",
    "https://medium.com/sicara/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f\n",
    "\n",
    "https://phoenixnap.com/kb/install-spark-on-windows-10\n",
    "\n",
    "https://github.com/steveloughran/winutils\n",
    "\n",
    "https://github.com/apache/spark/tree/master/examples/src/main/python\n",
    "\n",
    "https://spark.apache.org/examples.html\n",
    "\n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/sql/basic.py\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-statistics.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "https://towardsdatascience.com/spark-on-windows-a-getting-started-guide-11dc44412164\n",
    "\n",
    "https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873\n",
    "\n",
    "https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf\n",
    "\n",
    "https://deelesh.github.io/pyspark-windows.html\n",
    "\n",
    "https://changhsinlee.com/install-pyspark-windows-jupyter/\n",
    "\n",
    "https://medium.com/@naomi.fridman/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f\n",
    "\n",
    "https://www.dezyre.com/apache-spark-tutorial/tutorial-introduction-to-apache-spark\n",
    "\n",
    "https://medium.com/@naomi.fridman/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f\n",
    "\n",
    "https://github.com/naomifridman\n",
    "\n",
    "https://medium.com/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
